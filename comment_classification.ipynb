{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0eaede",
   "metadata": {},
   "source": [
    "# L'OrÃ©al Hackathon: Comment Clustering Strategy Implementation\n",
    "## Unsupervised Spam Detection and Quality Assessment using Gaussian Mixture Models\n",
    "\n",
    "This notebook implements the **comprehensive comment clustering strategy** using **Gaussian Mixture Models (GMM)** to identify spam and assess comment quality across diverse YouTube content. \n",
    "\n",
    "### ğŸ¯ **Clustering Objective**\n",
    "- **Primary Goal**: Use unsupervised clustering to separate YouTube comments into \"Spam\" and \"Quality\" clusters across ALL content types\n",
    "- **Secondary Goal**: Handle uncertainty with probabilistic thresholds (40-60% confidence flagged as uncertain)\n",
    "- **Key Innovation**: No labeled data required - discovers natural patterns in comment characteristics\n",
    "- **Dataset Scope**: Diverse YouTube content - beauty relevance â‰  spam indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90291a44",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 1: Import Required Libraries\n",
    "\n",
    "**Purpose**: Load all necessary libraries for unsupervised clustering, data processing, and visualization following our clustering strategy.\n",
    "\n",
    "**What we're importing**:\n",
    "- **Clustering**: Gaussian Mixture Models from scikit-learn for unsupervised spam detection\n",
    "- **Data handling**: pandas, numpy for processing diverse YouTube content\n",
    "- **Text processing**: NLTK, TextBlob for natural language processing across domains\n",
    "- **Visualization**: matplotlib, plotly for cluster analysis and uncertainty visualization\n",
    "- **Language support**: langdetect for multilingual comment support\n",
    "- **Emoji processing**: emoji library for cross-domain emoji analysis\n",
    "\n",
    "**Key Focus**: Set up environment for content-agnostic spam detection using probabilistic clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ee6d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scikit-learn imported successfully\n",
      "âœ… TextBlob imported successfully\n",
      "âœ… NLTK imported successfully\n",
      "âœ… Langdetect imported successfully\n",
      "âœ… Emoji library imported successfully\n",
      "âœ… Matplotlib and Seaborn imported successfully\n",
      "âœ… Plotly imported successfully\n",
      "\n",
      "ğŸ‰ ALL LIBRARIES IMPORTED SUCCESSFULLY!\n",
      "==================================================\n",
      "ğŸ“Š Pandas version: 2.2.3\n",
      "ğŸ¤– Scikit-learn available: True\n",
      "ğŸŒ€ GMM clustering ready: âœ…\n",
      "ğŸ“ TextBlob ready: âœ…\n",
      "ğŸŒ Langdetect ready: âœ…\n",
      "ğŸ˜Š Emoji library ready: âœ…\n",
      "ğŸ“ˆ Plotly ready: âœ…\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "try:\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.decomposition import PCA\n",
    "    print(\"âœ… Scikit-learn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Scikit-learn import error: {e}\")\n",
    "\n",
    "# Text processing libraries\n",
    "import re\n",
    "import string\n",
    "\n",
    "# TextBlob\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    print(\"âœ… TextBlob imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ Installing textblob...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"textblob\"])\n",
    "    from textblob import TextBlob\n",
    "    print(\"âœ… TextBlob installed and imported\")\n",
    "\n",
    "# NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    print(\"âœ… NLTK imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ Installing nltk...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "    print(\"âœ… NLTK installed and imported\")\n",
    "\n",
    "# Language detection\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0  # For consistent results\n",
    "    print(\"âœ… Langdetect imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ Installing langdetect...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"langdetect\"])\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "    print(\"âœ… Langdetect installed and imported\")\n",
    "\n",
    "# Emoji processing\n",
    "try:\n",
    "    import emoji\n",
    "    print(\"âœ… Emoji library imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ Installing emoji...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"emoji\"])\n",
    "    import emoji\n",
    "    print(\"âœ… Emoji library installed and imported\")\n",
    "\n",
    "# Visualization libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"âœ… Matplotlib and Seaborn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Matplotlib/Seaborn import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    print(\"âœ… Plotly imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ Installing plotly...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"plotly\"])\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    print(\"âœ… Plotly installed and imported\")\n",
    "\n",
    "# Utility libraries\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"\\nğŸ‰ ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ğŸ“Š Pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ¤– Scikit-learn available: {GaussianMixture is not None}\")\n",
    "print(f\"ğŸŒ€ GMM clustering ready: âœ…\")\n",
    "print(f\"ğŸ“ TextBlob ready: âœ…\")\n",
    "print(f\"ğŸŒ Langdetect ready: âœ…\")\n",
    "print(f\"ğŸ˜Š Emoji library ready: âœ…\")\n",
    "print(f\"ğŸ“ˆ Plotly ready: âœ…\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618a687",
   "metadata": {},
   "source": [
    "## ğŸ—‚ï¸ Step 2: Data Loading and Initial Exploration\n",
    "\n",
    "**Purpose**: Load the diverse YouTube comment datasets and perform initial exploration following our content-agnostic approach.\n",
    "\n",
    "**What this block does**:\n",
    "1. **Load comment files**: Read all 5 comment CSV files (comments1-5.csv) from diverse content\n",
    "2. **Load video context**: Read videos.csv for cross-domain context information  \n",
    "3. **Data inspection**: Check data shapes, columns, and diversity across content types\n",
    "4. **Memory optimization**: Plan batch processing for large datasets (>50MB each)\n",
    "\n",
    "**Key Philosophy**: \n",
    "- **Content-agnostic approach**: Comments span beauty, lifestyle, entertainment, education, etc.\n",
    "- **Spam definition**: Focus on bot-like behavior, not content relevance\n",
    "- **Preserve diversity**: Maintain all content types for comprehensive clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbf002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading comment data...\n",
      "   ğŸ“‚ Loading comments1.csv...\n",
      "   ğŸ“‚ Loading comments1.csv...\n",
      "   âœ… comments1.csv: 1,000,000 rows loaded\n",
      "   ğŸ“‚ Loading comments2.csv...\n",
      "   âœ… comments1.csv: 1,000,000 rows loaded\n",
      "   ğŸ“‚ Loading comments2.csv...\n",
      "   âœ… comments2.csv: 999,999 rows loaded\n",
      "   ğŸ“‚ Loading comments3.csv...\n",
      "   âœ… comments2.csv: 999,999 rows loaded\n",
      "   ğŸ“‚ Loading comments3.csv...\n",
      "   âœ… comments3.csv: 999,999 rows loaded\n",
      "   ğŸ“‚ Loading comments4.csv...\n",
      "   âœ… comments3.csv: 999,999 rows loaded\n",
      "   ğŸ“‚ Loading comments4.csv...\n",
      "   âœ… comments4.csv: 999,999 rows loaded\n",
      "   ğŸ“‚ Loading comments5.csv...\n",
      "   âœ… comments4.csv: 999,999 rows loaded\n",
      "   ğŸ“‚ Loading comments5.csv...\n",
      "   âœ… comments5.csv: 725,015 rows loaded\n",
      "   âœ… comments5.csv: 725,015 rows loaded\n",
      "\n",
      "âœ… All comments combined!\n",
      "ğŸ“Š Combined shape: (4725012, 10)\n",
      "\n",
      "ğŸ¥ Loading video data...\n",
      "\n",
      "âœ… All comments combined!\n",
      "ğŸ“Š Combined shape: (4725012, 10)\n",
      "\n",
      "ğŸ¥ Loading video data...\n",
      "âœ… Videos loaded!\n",
      "ğŸ“Š Videos shape: (92759, 15)\n",
      "\n",
      "ğŸ‰ DATA LOADING COMPLETE!\n",
      "   ğŸ’¬ Comments: 4,725,012 rows\n",
      "   ğŸ¥ Videos: 92,759 rows\n",
      "   ğŸ”— Unique videos in comments: 39,938\n",
      "   âœ… Ready for feature engineering!\n",
      "âœ… Videos loaded!\n",
      "ğŸ“Š Videos shape: (92759, 15)\n",
      "\n",
      "ğŸ‰ DATA LOADING COMPLETE!\n",
      "   ğŸ’¬ Comments: 4,725,012 rows\n",
      "   ğŸ¥ Videos: 92,759 rows\n",
      "   ğŸ”— Unique videos in comments: 39,938\n",
      "   âœ… Ready for feature engineering!\n"
     ]
    }
   ],
   "source": [
    "# Clear output and load data\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Clear any previous output to prevent duplication display\n",
    "clear_output(wait=True)\n",
    "\n",
    "data_path = r'c:\\Users\\user\\OneDrive\\Documents\\LOREAL HACKATHON\\dataset' \n",
    "comment_files = [f'comments{i}.csv' for i in range(1, 6)]\n",
    "video_file = 'videos.csv'\n",
    "\n",
    "print(\"ğŸ”„ Loading comment data...\")\n",
    "\n",
    "# Load all comment files\n",
    "all_comments = []\n",
    "for i, file in enumerate(comment_files, 1):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    print(f\"   ğŸ“‚ Loading {file}...\")\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        low_memory=False,       # prevents chunk guessing\n",
    "        dtype=str,              # read everything as string (fast, safe)\n",
    "    )\n",
    "    all_comments.append(df)\n",
    "    print(f\"   âœ… {file}: {len(df):,} rows loaded\")\n",
    "\n",
    "# Combine all comments\n",
    "comments_df = pd.concat(all_comments, ignore_index=True)\n",
    "print(f\"\\nâœ… All comments combined!\")\n",
    "print(f\"ğŸ“Š Combined shape: {comments_df.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ¥ Loading video data...\")\n",
    "try:\n",
    "    videos_df = pd.read_csv(os.path.join(data_path, video_file))\n",
    "    print(f\"âœ… Videos loaded!\")\n",
    "    print(f\"ğŸ“Š Videos shape: {videos_df.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading videos: {e}\")\n",
    "    videos_df = None\n",
    "\n",
    "print(f\"\\nğŸ‰ DATA LOADING COMPLETE!\")\n",
    "print(f\"   ğŸ’¬ Comments: {comments_df.shape[0]:,} rows\")\n",
    "print(f\"   ğŸ¥ Videos: {videos_df.shape[0]:,} rows\")\n",
    "print(f\"   ğŸ”— Unique videos in comments: {comments_df['videoId'].nunique():,}\")\n",
    "print(f\"   âœ… Ready for feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2810e",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Step 3: Text Preprocessing Functions\n",
    "\n",
    "**Purpose**: Create utility functions for cleaning and preprocessing text data following our content-agnostic clustering strategy.\n",
    "\n",
    "**What these functions do**:\n",
    "1. **`clean_text()`**: Light cleaning preserving domain-specific content (beauty, gaming, food, etc.)\n",
    "2. **`detect_language_safe()`**: Safely detect comment language across international content\n",
    "3. **`extract_emoji_features()`**: Extract emoji patterns for spam detection across all domains  \n",
    "4. **`calculate_text_stats()`**: Calculate text statistics for bot-like behavior detection\n",
    "\n",
    "**Key Philosophy**:\n",
    "- **Preserve content diversity**: Don't remove domain-specific terminology\n",
    "- **Focus on behavior patterns**: Detect spam through patterns, not content type\n",
    "- **Cross-domain emoji analysis**: Emojis relevant to respective content (ğŸµ for music, ğŸ• for food)\n",
    "- **Language-agnostic approach**: Behavioral patterns work across all languages without translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c19c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing preprocessing functions with multilingual support...\n",
      "\n",
      "Test 1: This is amazing! ğŸ˜âœ¨ Love this tutorial ğŸ’„ğŸ‘ Check out my channel: https://example.com\n",
      "   ğŸŒ Language: en\n",
      "   ğŸ”„ Translated: This is amazing! ğŸ˜âœ¨ Love this tutorial ğŸ’„ğŸ‘ Check out my channel: https://example.com\n",
      "   âœ… Translation attempted: False\n",
      "   ğŸ“ Cleaned: This is amazing! ğŸ˜âœ¨ Love this tutorial ğŸ’„ğŸ‘ Check out my channel: https://example.com\n",
      "   ğŸ˜Š Emojis: 4 total, sentiment: 3.1\n",
      "   ğŸ“Š Stats: 83 chars, 13 words\n",
      "\n",
      "Test 2: Â¡Esto es increÃ­ble! Me encanta este tutorial ğŸ’„\n",
      "   ğŸŒ Language: unknown\n",
      "   ğŸ”„ Translated: Â¡Esto es increÃ­ble! Me encanta este tutorial ğŸ’„\n",
      "   âœ… Translation attempted: False\n",
      "   ğŸ“ Cleaned: Â¡Esto es increÃ­ble! Me encanta este tutorial ğŸ’„\n",
      "   ğŸ˜Š Emojis: 1 total, sentiment: 0.1\n",
      "   ğŸ“Š Stats: 46 chars, 8 words\n",
      "\n",
      "Test 3: C'est magnifique! J'adore ce tutoriel beautÃ© ğŸ’‹\n",
      "   ğŸŒ Language: unknown\n",
      "   ğŸ”„ Translated: C'est magnifique! J'adore ce tutoriel beautÃ© ğŸ’‹\n",
      "   âœ… Translation attempted: False\n",
      "   ğŸ“ Cleaned: C'est magnifique! J'adore ce tutoriel beautÃ© ğŸ’‹\n",
      "   ğŸ˜Š Emojis: 1 total, sentiment: 0.1\n",
      "   ğŸ“Š Stats: 46 chars, 7 words\n",
      "\n",
      "Test 4: ç´ æ™´ã‚‰ã—ã„ï¼ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãŒå¤§å¥½ãã§ã™ âœ¨\n",
      "   ğŸŒ Language: unknown\n",
      "   ğŸ”„ Translated: ç´ æ™´ã‚‰ã—ã„ï¼ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãŒå¤§å¥½ãã§ã™ âœ¨\n",
      "   âœ… Translation attempted: False\n",
      "   ğŸ“ Cleaned: ç´ æ™´ã‚‰ã—ã„ï¼ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãŒå¤§å¥½ãã§ã™ âœ¨\n",
      "   ğŸ˜Š Emojis: 1 total, sentiment: 1.0\n",
      "   ğŸ“Š Stats: 23 chars, 2 words\n",
      "\n",
      "Test 5: Ù…Ø°Ù‡Ù„! Ø£Ø­Ø¨ Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ğŸ’•\n",
      "   ğŸŒ Language: unknown\n",
      "   ğŸ”„ Translated: Ù…Ø°Ù‡Ù„! Ø£Ø­Ø¨ Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ğŸ’•\n",
      "   âœ… Translation attempted: False\n",
      "   ğŸ“ Cleaned: Ù…Ø°Ù‡Ù„! Ø£Ø­Ø¨ Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ğŸ’•\n",
      "   ğŸ˜Š Emojis: 1 total, sentiment: 1.0\n",
      "   ğŸ“Š Stats: 32 chars, 6 words\n",
      "\n",
      "\n",
      "âœ… All preprocessing functions with translation created successfully!\n",
      "\n",
      "ğŸŒ Translation Features:\n",
      "   ğŸ”„ Auto-translate non-English comments to English\n",
      "   ğŸŒ Preserve original text for emoji analysis\n",
      "   ğŸ“Š Enable content-agnostic spam detection across languages\n",
      "   ğŸ¯ Maintain cultural context while standardizing text analysis\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text while preserving meaningful content.\n",
    "    \n",
    "    This function performs light cleaning to standardize text without\n",
    "    removing important information like emojis or context.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string and handle encoding issues\n",
    "    text = str(text)\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove excessive punctuation (more than 3 consecutive)\n",
    "    text = re.sub(r'[!]{4,}', '!!!', text)\n",
    "    text = re.sub(r'[?]{4,}', '???', text)\n",
    "    text = re.sub(r'[.]{4,}', '...', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def detect_language_safe(text, confidence_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Safely detect language with confidence scoring.\n",
    "    \n",
    "    Returns language code if detection confidence is high enough,\n",
    "    otherwise returns 'unknown'. Used for metadata only.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(str(text).strip()) < 3:  # Too short to detect\n",
    "            return 'unknown'\n",
    "        \n",
    "        # Remove emojis for language detection\n",
    "        text_no_emoji = emoji.demojize(str(text))\n",
    "        \n",
    "        detected_lang = detect(text_no_emoji)\n",
    "        return detected_lang\n",
    "    \n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def calculate_text_stats(text):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive text statistics.\n",
    "    \n",
    "    Returns various text metrics useful for spam detection.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Basic stats\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    \n",
    "    # Advanced stats\n",
    "    avg_word_length = np.mean([len(word) for word in text.split()]) if word_count > 0 else 0\n",
    "    \n",
    "    # Capitalization analysis\n",
    "    upper_count = sum(1 for c in text if c.isupper())\n",
    "    caps_ratio = upper_count / max(char_count, 1)\n",
    "    \n",
    "    # Special character analysis\n",
    "    special_chars = sum(1 for c in text if c in string.punctuation)\n",
    "    special_ratio = special_chars / max(char_count, 1)\n",
    "    \n",
    "    # URL detection\n",
    "    url_count = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "    \n",
    "    # Repetition detection (same word repeated)\n",
    "    words = text.lower().split()\n",
    "    word_freq = Counter(words)\n",
    "    max_word_freq = max(word_freq.values()) if words else 0\n",
    "    repetition_ratio = max_word_freq / max(word_count, 1)\n",
    "    \n",
    "    return {\n",
    "        'char_count': char_count,\n",
    "        'word_count': word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'special_ratio': special_ratio,\n",
    "        'url_count': url_count,\n",
    "        'repetition_ratio': repetition_ratio,\n",
    "        'has_url': 1 if url_count > 0 else 0\n",
    "    }\n",
    "\n",
    "def extract_emoji_features(text):\n",
    "    \"\"\"\n",
    "    Extract comprehensive emoji features for spam detection.\n",
    "    \n",
    "    Returns dictionary with emoji statistics and spam indicators.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Extract all emojis\n",
    "    emojis = emoji.emoji_list(text)\n",
    "    emoji_chars = [item['emoji'] for item in emojis]\n",
    "    \n",
    "    # Basic emoji stats\n",
    "    emoji_count = len(emoji_chars)\n",
    "    unique_emojis = len(set(emoji_chars))\n",
    "    text_length = len(text)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    emoji_ratio = emoji_count / max(text_length, 1)\n",
    "    emoji_diversity = unique_emojis / max(emoji_count, 1)\n",
    "    \n",
    "    # EXPANDED sentiment emojis (broader coverage)\n",
    "    positive_emojis = ['ğŸ˜', 'ğŸ’•', 'â¤ï¸', 'âœ¨', 'ğŸŒŸ', 'ğŸ˜Š', 'ğŸ‘', 'ğŸ”¥', 'ğŸ’¯', 'ğŸ¥°', 'ğŸ˜˜',\n",
    "                       'ğŸ¤©', 'ğŸ˜', 'ğŸ¥³', 'ğŸ¤—', 'ğŸ˜‡', 'ğŸ™Œ', 'ğŸ‘', 'ğŸ’ª', 'ğŸ‰', 'ğŸ™', 'â˜ºï¸', \n",
    "                       'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜†', 'ğŸ¥²', 'ğŸ’–', 'ğŸ’—', 'ğŸ’']\n",
    "    \n",
    "    negative_emojis = ['ğŸ˜’', 'ğŸ˜•', 'ğŸ‘', 'ğŸ˜', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ’”', 'ğŸ˜¢', 'ğŸ˜­', 'ğŸ™„', \n",
    "                       'ğŸ˜¤', 'ğŸ˜©', 'ğŸ¤®', 'ğŸ¤¢', 'ğŸ˜µ', 'ğŸ¥º', 'ğŸ˜–', 'ğŸ˜“', 'ğŸ˜°', 'ğŸ˜¨',\n",
    "                       'ğŸ˜±', 'ğŸ¤¬', 'ğŸ˜ˆ', 'ğŸ’€', 'ğŸ˜·', 'ğŸ¤’', 'ğŸ¤•']\n",
    "    \n",
    "    # Count sentiment emojis\n",
    "    positive_count = sum([emoji_chars.count(e) for e in positive_emojis])\n",
    "    negative_count = sum([emoji_chars.count(e) for e in negative_emojis])\n",
    "    \n",
    "    # Count unknown/neutral emojis (not in positive or negative lists)\n",
    "    known_emojis = set(positive_emojis + negative_emojis)\n",
    "    unknown_emoji_count = len([e for e in emoji_chars if e not in known_emojis])\n",
    "    \n",
    "    # Content category emojis\n",
    "    music_emojis = ['ğŸµ', 'ğŸ¤', 'ğŸ¸', 'ğŸ¼', 'ğŸ¶', 'ğŸ§', 'ğŸ¹', 'ğŸ¥', 'ğŸº', 'ğŸ»']\n",
    "    food_emojis = ['ğŸ•', 'ğŸ”', 'ğŸ¥—', 'ğŸ°', 'ğŸœ', 'ğŸ', 'ğŸŒ', 'ğŸ¥‘', 'ğŸ“', 'ğŸ‰', 'ğŸ§']\n",
    "    beauty_emojis = ['ğŸ’„', 'ğŸ’‹', 'ğŸ‘„', 'ğŸ’…', 'ğŸ§´', 'ğŸª', 'âœ¨', 'ğŸ’', 'ğŸ‘—', 'ğŸ‘ ']\n",
    "    \n",
    "    music_count = sum([emoji_chars.count(e) for e in music_emojis])\n",
    "    food_count = sum([emoji_chars.count(e) for e in food_emojis])\n",
    "    beauty_count = sum([emoji_chars.count(e) for e in beauty_emojis])\n",
    "    \n",
    "    # Enhanced sentiment scoring (handles unknown emojis)\n",
    "    # Give small positive weight to unknown emojis (engagement indicator)\n",
    "    sentiment_score = positive_count - negative_count + (unknown_emoji_count * 0.1)\n",
    "    \n",
    "    return {\n",
    "        'emoji_count': emoji_count,\n",
    "        'emoji_ratio': emoji_ratio,\n",
    "        'emoji_diversity': emoji_diversity,\n",
    "        'positive_emoji_count': positive_count,\n",
    "        'negative_emoji_count': negative_count,\n",
    "        'unknown_emoji_count': unknown_emoji_count,\n",
    "        'emoji_sentiment_score': sentiment_score,\n",
    "        'music_emoji_count': music_count,\n",
    "        'food_emoji_count': food_count,\n",
    "        'beauty_emoji_count': beauty_count,\n",
    "        'spam_emoji_indicator': 1 if emoji_ratio > 0.3 and emoji_diversity < 0.5 else 0\n",
    "    }\n",
    "\n",
    "# Test the functions with sample comments (no translation needed)\n",
    "test_comments = [\n",
    "    \"This is amazing! ğŸ˜âœ¨ Love this tutorial ğŸ’„ğŸ‘ Check out my channel: https://example.com\",\n",
    "    \"Â¡Esto es increÃ­ble! Me encanta este tutorial ğŸ’„\",\n",
    "    \"C'est magnifique! J'adore ce tutoriel beautÃ© ğŸ’‹\",\n",
    "    \"ç´ æ™´ã‚‰ã—ã„ï¼ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãŒå¤§å¥½ãã§ã™ âœ¨\",\n",
    "    \"Ù…Ø°Ù‡Ù„! Ø£Ø­Ø¨ Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ğŸ’•\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing preprocessing functions (language-agnostic approach)...\\n\")\n",
    "\n",
    "for i, comment in enumerate(test_comments, 1):\n",
    "    print(f\"Test {i}: {comment}\")\n",
    "    \n",
    "    # Test language detection (metadata only)\n",
    "    detected_lang = detect_language_safe(comment)\n",
    "    print(f\"   ğŸŒ Language: {detected_lang}\")\n",
    "    \n",
    "    # Test other functions on original text\n",
    "    cleaned = clean_text(comment)\n",
    "    emoji_features = extract_emoji_features(comment)\n",
    "    text_stats = calculate_text_stats(comment)\n",
    "    \n",
    "    print(f\"   ğŸ“ Cleaned: {cleaned}\")\n",
    "    print(f\"   ğŸ˜Š Emojis: {emoji_features['emoji_count']} total, sentiment: {emoji_features['emoji_sentiment_score']}\")\n",
    "    print(f\"   ğŸ“Š Stats: {text_stats['char_count']} chars, {text_stats['word_count']} words\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nâœ… All preprocessing functions created successfully!\")\n",
    "print(\"\\n\udfaf Language-Agnostic Features:\")\n",
    "print(\"   \udcdd Text: Bot-like patterns work across all languages\")\n",
    "print(\"   ğŸ˜Š Emoji: Universal emotional expression patterns\") \n",
    "print(\"   ğŸ“Š Engagement: Behavioral patterns are language-independent\")\n",
    "print(\"   \udf0d Language: Detected for metadata only (no translation needed)\")\n",
    "print(\"   ğŸš€ Result: Fast, effective spam detection without translation overhead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98b375",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 4: Feature Engineering Pipeline\n",
    "\n",
    "**Purpose**: Create a comprehensive feature engineering pipeline following our content-agnostic clustering strategy.\n",
    "\n",
    "**What this pipeline does**:\n",
    "1. **Text complexity analysis**: Extract patterns that differentiate spam from genuine engagement\n",
    "2. **Cross-domain emoji analysis**: Process emojis contextually (music ğŸµ, food ğŸ•, beauty ğŸ’„)\n",
    "3. **Engagement authenticity**: Identify suspicious engagement patterns across all content types\n",
    "4. **Behavioral indicators**: Focus on bot-like patterns rather than content relevance\n",
    "5. **Context preservation**: Maintain video context for relevance assessment\n",
    "\n",
    "**Key Features for GMM Clustering**:\n",
    "- **Text-based**: Complexity, repetition patterns, generic responses\n",
    "- **Emoji & Symbol**: Sentiment, domain-relevance, spam patterns  \n",
    "- **Engagement**: Authenticity indicators, suspicious patterns\n",
    "- **Context Relevance**: General video context matching (not domain-specific)\n",
    "- **Language & Cultural**: Cross-cultural engagement patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ee9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_pipeline(comments_df, videos_df=None):\n",
    "    \"\"\"\n",
    "    Content-agnostic feature engineering pipeline for GMM clustering.\n",
    "    \n",
    "    This function processes comments from diverse domains (beauty, gaming, food, education)\n",
    "    and languages, extracting behavioral patterns for spam detection without translation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”§ Processing {len(comments_df):,} comments with language-agnostic approach...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize feature dataframe\n",
    "    features_df = comments_df.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. BEHAVIORAL TEXT PROCESSING (ORIGINAL TEXT)\n",
    "    # ========================================\n",
    "    print(\"   ğŸ“ Processing behavioral text features...\")\n",
    "    \n",
    "    # Light cleaning preserving domain-specific content\n",
    "    features_df['cleaned_text'] = features_df['textOriginal'].apply(clean_text)\n",
    "    \n",
    "    # Language detection for metadata only\n",
    "    features_df['original_language'] = features_df['textOriginal'].apply(detect_language_safe)\n",
    "    features_df['is_english'] = (features_df['original_language'] == 'en').astype(int)\n",
    "    \n",
    "    # Language diversity features (metadata only)\n",
    "    features_df['is_major_language'] = features_df['original_language'].isin(\n",
    "        ['en', 'es', 'fr', 'de', 'it', 'pt', 'ja', 'ko', 'zh', 'ar', 'hi', 'ru']\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"      ğŸ“Š Languages detected: {features_df['original_language'].nunique()}\")\n",
    "    \n",
    "    # Text statistics for bot detection (works on original text)\n",
    "    text_stats = features_df['cleaned_text'].apply(calculate_text_stats)\n",
    "    text_stats_df = pd.DataFrame(text_stats.tolist())\n",
    "    features_df = pd.concat([features_df, text_stats_df], axis=1)\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. CROSS-DOMAIN EMOJI ANALYSIS\n",
    "    # ========================================\n",
    "    print(\"   ğŸ˜Š Processing cross-domain emoji features...\")\n",
    "    \n",
    "    emoji_features = features_df['textOriginal'].apply(extract_emoji_features)\n",
    "    emoji_features_df = pd.DataFrame(emoji_features.tolist())\n",
    "    features_df = pd.concat([features_df, emoji_features_df], axis=1)\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. ENGAGEMENT AUTHENTICITY (CONTENT-AGNOSTIC)\n",
    "    # ========================================\n",
    "    print(\"   ğŸ“Š Processing engagement authenticity features...\")\n",
    "    \n",
    "    # Basic engagement metrics\n",
    "    features_df['likeCount'] = pd.to_numeric(features_df['likeCount'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Content-agnostic engagement patterns\n",
    "    if videos_df is not None:\n",
    "        # Merge with video data for context (any content type)\n",
    "        video_engagement = videos_df.groupby('videoId').agg({\n",
    "            'viewCount': 'first',\n",
    "            'likeCount': 'first',\n",
    "            'commentCount': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        video_engagement = video_engagement.rename(columns={'likeCount': 'video_likeCount'})\n",
    "        features_df = features_df.merge(video_engagement, on='videoId', how='left')\n",
    "        \n",
    "        # Calculate universal engagement ratios\n",
    "        features_df['comment_to_video_like_ratio'] = features_df['likeCount'] / (features_df['video_likeCount'] + 1)\n",
    "        features_df['likes_per_char'] = features_df['likeCount'] / (features_df['char_count'] + 1)\n",
    "    else:\n",
    "        features_df['likes_per_char'] = features_df['likeCount'] / (features_df['char_count'] + 1)\n",
    "    \n",
    "    # Reply structure (universal pattern)\n",
    "    features_df['is_reply'] = (~features_df['parentCommentId'].isna()).astype(int)\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. TEMPORAL PATTERNS (UNIVERSAL)\n",
    "    # ========================================\n",
    "    print(\"   ğŸ• Processing temporal patterns...\")\n",
    "    \n",
    "    # Convert dates\n",
    "    features_df['publishedAt'] = pd.to_datetime(features_df['publishedAt'], errors='coerce')\n",
    "    \n",
    "    # Extract universal time features\n",
    "    features_df['hour_of_day'] = features_df['publishedAt'].dt.hour\n",
    "    features_df['day_of_week'] = features_df['publishedAt'].dt.dayofweek\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. LANGUAGE-AGNOSTIC SPAM BEHAVIOR DETECTION\n",
    "    # ========================================\n",
    "    print(\"   ğŸš« Processing language-agnostic spam behavior indicators...\")\n",
    "    \n",
    "    # Generic comment detection (works on original text with regex)\n",
    "    generic_patterns = [\n",
    "        r'^(first|1st)!?$',\n",
    "        r'^(nice|good|great|awesome|amazing|cool)!*$',\n",
    "        r'^(love it|love this|loved it)!*$',\n",
    "        r'^(thanks|thank you)!*$',\n",
    "        r'^(wow|omg|lol|haha)!*$'\n",
    "    ]\n",
    "    \n",
    "    features_df['is_generic'] = 0\n",
    "    for pattern in generic_patterns:\n",
    "        mask = features_df['cleaned_text'].str.lower().str.match(pattern, na=False)\n",
    "        features_df.loc[mask, 'is_generic'] = 1\n",
    "    \n",
    "    # Suspicious engagement patterns (universal)\n",
    "    features_df['suspicious_engagement'] = (\n",
    "        (features_df['char_count'] < 10) & \n",
    "        (features_df['likeCount'] > 5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Bot-like behavior indicators\n",
    "    features_df['excessive_caps'] = (features_df['caps_ratio'] > 0.5).astype(int)\n",
    "    features_df['excessive_repetition'] = (features_df['repetition_ratio'] > 0.7).astype(int)\n",
    "    \n",
    "    # Short non-English spam indicator (behavioral, not content-based)\n",
    "    features_df['short_non_english_spam'] = (\n",
    "        (features_df['is_english'] == 0) & \n",
    "        (features_df['char_count'] < 15) & \n",
    "        (features_df['emoji_count'] == 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # ========================================\n",
    "    # 6. QUALITY INDICATORS (UNIVERSAL)\n",
    "    # ========================================\n",
    "    print(\"   â­ Processing universal quality indicators...\")\n",
    "    \n",
    "    # Minimum viable engagement\n",
    "    features_df['sufficient_length'] = (features_df['char_count'] >= 10).astype(int)\n",
    "    \n",
    "    # Balanced communication patterns\n",
    "    features_df['balanced_punctuation'] = (\n",
    "        (features_df['special_ratio'] > 0.01) & \n",
    "        (features_df['special_ratio'] < 0.3)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Meaningful emoji usage (cross-domain)\n",
    "    features_df['meaningful_emoji_usage'] = (\n",
    "        (features_df['emoji_count'] > 0) & \n",
    "        (features_df['emoji_ratio'] < 0.3) & \n",
    "        (features_df['emoji_diversity'] > 0.5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Authentic engagement indicator\n",
    "    features_df['authentic_engagement'] = (\n",
    "        (features_df['char_count'] >= 15) &\n",
    "        (features_df['caps_ratio'] < 0.4) &\n",
    "        (features_df['repetition_ratio'] < 0.5) &\n",
    "        (features_df['special_ratio'] < 0.25)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Cross-cultural engagement (non-English but substantial)\n",
    "    features_df['substantial_non_english'] = (\n",
    "        (features_df['is_english'] == 0) & \n",
    "        (features_df['char_count'] >= 20) & \n",
    "        (features_df['emoji_count'] > 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # ========================================\n",
    "    # 7. FINAL FEATURE SELECTION FOR GMM\n",
    "    # ========================================\n",
    "    \n",
    "    # Content-agnostic features for clustering (behavioral + language metadata)\n",
    "    feature_columns = [\n",
    "        # Behavioral text features (language-agnostic)\n",
    "        'char_count', 'word_count', 'avg_word_length',\n",
    "        'caps_ratio', 'special_ratio', 'repetition_ratio',\n",
    "        \n",
    "        # Cross-domain emoji features\n",
    "        'emoji_count', 'emoji_ratio', 'emoji_diversity',\n",
    "        'emoji_sentiment_score', 'spam_emoji_indicator',\n",
    "        'beauty_emoji_count', 'music_emoji_count', 'food_emoji_count',\n",
    "        \n",
    "        # Universal engagement features\n",
    "        'likeCount', 'likes_per_char', 'is_reply',\n",
    "        \n",
    "        # Temporal patterns\n",
    "        'hour_of_day', 'day_of_week',\n",
    "        \n",
    "        # Language-agnostic spam behavior indicators\n",
    "        'is_generic', 'suspicious_engagement', 'excessive_caps',\n",
    "        'excessive_repetition', 'has_url', 'url_count',\n",
    "        'short_non_english_spam',\n",
    "        \n",
    "        # Language metadata (not translated content)\n",
    "        'is_english', 'is_major_language', 'substantial_non_english',\n",
    "        \n",
    "        # Universal quality indicators\n",
    "        'sufficient_length', 'balanced_punctuation', \n",
    "        'meaningful_emoji_usage', 'authentic_engagement'\n",
    "    ]\n",
    "    \n",
    "    # Add video context features if available\n",
    "    if 'comment_to_video_like_ratio' in features_df.columns:\n",
    "        feature_columns.append('comment_to_video_like_ratio')\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in feature_columns:\n",
    "        if col in features_df.columns:\n",
    "            features_df[col] = features_df[col].fillna(0)\n",
    "    \n",
    "    # Create final feature matrix\n",
    "    available_features = [col for col in feature_columns if col in features_df.columns]\n",
    "    X = features_df[available_features].copy()\n",
    "    \n",
    "    # Use cleaned original text for analysis\n",
    "    processed_data = {\n",
    "        'features': X,\n",
    "        'text': features_df['cleaned_text'],  # Use cleaned original text\n",
    "        'original_data': features_df,\n",
    "        'feature_names': available_features\n",
    "    }\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"   âœ… Language-agnostic feature engineering completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"   ğŸ“Š Generated {len(available_features)} behavioral features\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "print(\"âœ… Language-agnostic feature engineering pipeline created!\")\n",
    "print(\"\\nğŸ¯ Enhanced Features WITHOUT Translation:\")\n",
    "print(\"   ğŸ“ Text: Bot-like patterns analyzed on original text\")\n",
    "print(\"   ğŸ˜Š Emoji: Cross-cultural emoji analysis on original text\")\n",
    "print(\"   ğŸ“Š Engagement: Universal authenticity indicators\")\n",
    "print(\"   \udf0d Language: Detection for metadata only (no translation)\")\n",
    "print(\"   ğŸ• Temporal: Universal posting patterns\")\n",
    "print(\"   ğŸš« Spam: Language-agnostic behavioral patterns\")\n",
    "print(\"   â­ Quality: Cross-cultural meaningful engagement patterns\")\n",
    "print(\"   ğŸš€ Result: Fast, effective, language-agnostic spam detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4cd94",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Step 5: Load and Process Sample Data\n",
    "\n",
    "**Purpose**: Load a sample of diverse comment data to test our content-agnostic feature engineering pipeline.\n",
    "\n",
    "**What this block does**:\n",
    "1. **Sample loading**: Load first 10,000 comments from comments1.csv representing diverse content\n",
    "2. **Pipeline testing**: Run our cross-domain feature engineering pipeline on real data\n",
    "3. **Diversity analysis**: Examine features across different content types (beauty, gaming, food, etc.)\n",
    "4. **Quality check**: Verify that features capture behavioral patterns rather than content bias\n",
    "\n",
    "**Why start with a sample**: Testing with diverse content first helps us ensure our clustering approach works across all domains before processing the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data for testing\n",
    "print(\"ğŸ”„ Loading sample comment data...\")\n",
    "\n",
    "try:\n",
    "    # Load first batch of comments (sample size for testing)\n",
    "    sample_size = 10000\n",
    "    comments_sample = pd.read_csv(\n",
    "        os.path.join(data_path, 'comments1.csv'), \n",
    "        nrows=sample_size\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(comments_sample):,} sample comments\")\n",
    "    print(f\"ğŸ“‹ Columns: {list(comments_sample.columns)}\")\n",
    "    \n",
    "    # Display basic info about the sample\n",
    "    print(f\"\\nğŸ“Š Sample Data Overview:\")\n",
    "    print(f\"   ğŸ’¬ Comments: {len(comments_sample):,}\")\n",
    "    print(f\"   ğŸ¥ Unique videos: {comments_sample['videoId'].nunique():,}\")\n",
    "    print(f\"   ğŸ‘¥ Unique authors: {comments_sample['authorId'].nunique():,}\")\n",
    "    print(f\"   ğŸ’ Total likes: {comments_sample['likeCount'].sum():,}\")\n",
    "    \n",
    "    # Show sample comments\n",
    "    print(f\"\\nğŸ“ Sample Comments:\")\n",
    "    for i in range(3):\n",
    "        comment = comments_sample['textOriginal'].iloc[i]\n",
    "        likes = comments_sample['likeCount'].iloc[i]\n",
    "        print(f\"   {i+1}. \\\"{comment[:100]}...\\\" (â¤ï¸ {likes} likes)\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nğŸ” Missing Values:\")\n",
    "    missing_counts = comments_sample.isnull().sum()\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"   {col}: {count:,} ({count/len(comments_sample)*100:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading sample data: {e}\")\n",
    "    comments_sample = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b42edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature engineering pipeline\n",
    "if comments_sample is not None:\n",
    "    print(\"ğŸ”§ Testing feature engineering pipeline...\\n\")\n",
    "    \n",
    "    # Process sample data through our pipeline\n",
    "    processed_data = create_feature_pipeline(comments_sample, videos_df)\n",
    "    \n",
    "    # Extract results\n",
    "    X_features = processed_data['features']\n",
    "    text_data = processed_data['text']\n",
    "    feature_names = processed_data['feature_names']\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Feature Engineering Results:\")\n",
    "    print(f\"   ğŸ“ˆ Feature matrix shape: {X_features.shape}\")\n",
    "    print(f\"   ğŸ“‹ Features generated: {len(feature_names)}\")\n",
    "    \n",
    "    # Display feature statistics\n",
    "    print(f\"\\nğŸ“ˆ Feature Statistics:\")\n",
    "    feature_stats = X_features.describe()\n",
    "    print(feature_stats.round(3))\n",
    "    \n",
    "    # Check for any issues\n",
    "    print(f\"\\nğŸ” Data Quality Check:\")\n",
    "    print(f\"   â“ Missing values: {X_features.isnull().sum().sum()}\")\n",
    "    print(f\"   â™¾ï¸ Infinite values: {np.isinf(X_features).sum().sum()}\")\n",
    "    print(f\"   ğŸ“Š All numeric: {X_features.dtypes.apply(lambda x: np.issubdtype(x, np.number)).all()}\")\n",
    "    \n",
    "    # Show sample of features\n",
    "    print(f\"\\nğŸ“ Sample Feature Values (first 3 comments):\")\n",
    "    sample_features = X_features.head(3)\n",
    "    for i, (idx, row) in enumerate(sample_features.iterrows()):\n",
    "        print(f\"\\n   Comment {i+1}: \\\"{text_data.iloc[i][:60]}...\\\"\")\n",
    "        print(f\"   Features: char_count={row['char_count']}, emoji_count={row['emoji_count']}, \")\n",
    "        print(f\"            likeCount={row['likeCount']}, is_generic={row['is_generic']}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Pipeline testing completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot test pipeline - sample data not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5973262",
   "metadata": {},
   "source": [
    "## ğŸŒ€ Step 6: Prepare Data for GMM Clustering with Uncertainty Handling\n",
    "\n",
    "**Purpose**: Prepare our engineered features for Gaussian Mixture Model clustering following our probabilistic uncertainty strategy.\n",
    "\n",
    "**What this approach does**:\n",
    "1. **Feature scaling**: Normalize features for optimal GMM performance across all domains\n",
    "2. **Feature selection**: Choose most discriminative features for content-agnostic spam detection\n",
    "3. **Uncertainty framework**: Prepare for 3-way classification (spam/quality/uncertain)\n",
    "4. **Cross-domain validation**: Ensure features work across beauty, gaming, food, education content\n",
    "5. **Data quality checks**: Validate data for unsupervised learning\n",
    "\n",
    "**Uncertainty Thresholds**:\n",
    "- **Confident assignments**: >60% probability \n",
    "- **Uncertain zone**: 40-60% probability (flagged for manual review)\n",
    "- **Business value**: Avoids forced classifications of genuinely ambiguous comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc00697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_clustering_data(processed_data, scale_features=True):\n",
    "    \"\"\"\n",
    "    Prepare data for GMM clustering following content-agnostic strategy.\n",
    "    \n",
    "    This function prepares behavioral features for unsupervised clustering\n",
    "    to identify spam patterns across diverse content domains.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_features = processed_data['features']\n",
    "    text_data = processed_data['text']\n",
    "    original_data = processed_data['original_data']\n",
    "    \n",
    "    print(\"ğŸŒ€ Preparing data for content-agnostic GMM clustering...\\n\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. CONTENT-AGNOSTIC FEATURE SELECTION\n",
    "    # ========================================\n",
    "    \n",
    "    # Select features that work across all content domains\n",
    "    clustering_features = [\n",
    "        # Behavioral text complexity (universal)\n",
    "        'char_count', 'word_count', 'avg_word_length',\n",
    "        \n",
    "        # Bot-like behavior indicators (universal)\n",
    "        'caps_ratio', 'special_ratio', 'repetition_ratio',\n",
    "        'is_generic', 'excessive_caps', 'excessive_repetition', 'url_count',\n",
    "        \n",
    "        # Cross-domain emoji patterns\n",
    "        'emoji_count', 'emoji_ratio', 'emoji_diversity',\n",
    "        'emoji_sentiment_score', 'spam_emoji_indicator',\n",
    "        \n",
    "        # Universal engagement authenticity\n",
    "        'likeCount', 'likes_per_char', 'is_reply',\n",
    "        'suspicious_engagement', 'authentic_engagement',\n",
    "        \n",
    "        # Quality indicators (work across domains)\n",
    "        'sufficient_length', 'balanced_punctuation', 'meaningful_emoji_usage'\n",
    "    ]\n",
    "    \n",
    "    # Add video context features if available\n",
    "    if 'comment_to_video_like_ratio' in X_features.columns:\n",
    "        clustering_features.append('comment_to_video_like_ratio')\n",
    "    \n",
    "    # Select available features\n",
    "    available_features = [f for f in clustering_features if f in X_features.columns]\n",
    "    X_clustering = X_features[available_features].copy()\n",
    "    \n",
    "    print(f\"ğŸ“Š Selected {len(available_features)} content-agnostic features:\")\n",
    "    print(\"   ğŸ¯ Behavioral patterns that work across beauty, gaming, food, education:\")\n",
    "    for i, feature in enumerate(available_features):\n",
    "        category = \"ğŸ¤– Bot-like\" if feature in ['caps_ratio', 'repetition_ratio', 'is_generic'] else \\\n",
    "                   \"ğŸ˜Š Emoji\" if 'emoji' in feature else \\\n",
    "                   \"ğŸ“Š Engagement\" if feature in ['likeCount', 'likes_per_char'] else \\\n",
    "                   \"â­ Quality\" if feature in ['sufficient_length', 'balanced_punctuation'] else \\\n",
    "                   \"ğŸ“ Text\"\n",
    "        print(f\"   {i+1:2d}. {feature:<25} ({category})\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. DATA QUALITY CHECKS\n",
    "    # ========================================\n",
    "    \n",
    "    print(f\"\\nğŸ” Data quality checks for clustering:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = X_clustering.isnull().sum().sum()\n",
    "    print(f\"   â“ Missing values: {missing_values}\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    infinite_values = np.isinf(X_clustering).sum().sum()\n",
    "    print(f\"   â™¾ï¸ Infinite values: {infinite_values}\")\n",
    "    \n",
    "    # Check feature variance (important for GMM)\n",
    "    zero_variance_features = X_clustering.columns[X_clustering.var() == 0].tolist()\n",
    "    if zero_variance_features:\n",
    "        print(f\"   âš ï¸ Zero variance features: {zero_variance_features}\")\n",
    "    \n",
    "    # Fill any remaining missing values\n",
    "    if missing_values > 0:\n",
    "        X_clustering = X_clustering.fillna(0)\n",
    "        print(f\"   âœ… Filled missing values with 0\")\n",
    "    \n",
    "    # Replace infinite values\n",
    "    if infinite_values > 0:\n",
    "        X_clustering = X_clustering.replace([np.inf, -np.inf], [1e6, -1e6])\n",
    "        print(f\"   âœ… Replaced infinite values\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. FEATURE SCALING FOR GMM\n",
    "    # ========================================\n",
    "    \n",
    "    if scale_features:\n",
    "        print(f\"\\nğŸ“ Scaling features for optimal GMM performance...\")\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_clustering)\n",
    "        X_clustering_final = pd.DataFrame(X_scaled, columns=available_features, index=X_clustering.index)\n",
    "        \n",
    "        print(f\"   âœ… Features standardized (mean=0, std=1)\")\n",
    "        \n",
    "        # Show scaling statistics\n",
    "        print(f\"   ğŸ“Š Scaled feature statistics:\")\n",
    "        print(f\"      ğŸ“ˆ Range: [{X_clustering_final.min().min():.3f}, {X_clustering_final.max().max():.3f}]\")\n",
    "        print(f\"      ğŸ“Š Mean: {X_clustering_final.mean().mean():.3f}\")\n",
    "        print(f\"      ğŸ“ Std: {X_clustering_final.std().mean():.3f}\")\n",
    "        \n",
    "    else:\n",
    "        scaler = None\n",
    "        X_clustering_final = X_clustering\n",
    "        print(f\"\\nğŸ“ Using raw features (no scaling)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. FEATURE CORRELATION ANALYSIS\n",
    "    # ========================================\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Analyzing feature relationships for GMM...\")\n",
    "    \n",
    "    correlation_matrix = X_clustering_final.corr()\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i, col1 in enumerate(correlation_matrix.columns):\n",
    "        for j, col2 in enumerate(correlation_matrix.columns):\n",
    "            if i < j and abs(correlation_matrix.loc[col1, col2]) > 0.8:\n",
    "                high_corr_pairs.append((col1, col2, correlation_matrix.loc[col1, col2]))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"   âš ï¸ High correlations detected (>0.8):\")\n",
    "        for col1, col2, corr in high_corr_pairs[:3]:  # Show top 3\n",
    "            print(f\"      ğŸ“Š {col1} â†” {col2}: {corr:.3f}\")\n",
    "        if len(high_corr_pairs) > 3:\n",
    "            print(f\"      ... and {len(high_corr_pairs)-3} more\")\n",
    "    else:\n",
    "        print(f\"   âœ… No problematic feature correlations detected\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. CLUSTERING READINESS SUMMARY\n",
    "    # ========================================\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Data ready for content-agnostic GMM clustering:\")\n",
    "    print(f\"   ğŸ“Š Shape: {X_clustering_final.shape}\")\n",
    "    print(f\"   ğŸ“‹ Features: {len(available_features)} behavioral indicators\")\n",
    "    print(f\"   ğŸ’¬ Comments: {len(X_clustering_final):,} from diverse content\")\n",
    "    print(f\"   ğŸŒ€ Ready for: 2-component GMM with uncertainty handling\")\n",
    "    \n",
    "    return {\n",
    "        'X_clustering': X_clustering_final,\n",
    "        'text_data': text_data,\n",
    "        'original_data': original_data,\n",
    "        'feature_names': available_features,\n",
    "        'scaler': scaler,\n",
    "        'data_stats': {\n",
    "            'n_samples': len(X_clustering_final),\n",
    "            'n_features': len(available_features),\n",
    "            'missing_values': missing_values,\n",
    "            'infinite_values': infinite_values,\n",
    "            'high_correlations': len(high_corr_pairs)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Apply clustering data preparation\n",
    "if 'processed_data' in locals():\n",
    "    clustering_data = prepare_clustering_data(processed_data, scale_features=True)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Content-Agnostic Clustering Data Summary:\")\n",
    "    print(f\"   ğŸ¯ Ready for GMM with uncertainty handling\")\n",
    "    print(f\"   ğŸ“ˆ Shape: {clustering_data['X_clustering'].shape}\")\n",
    "    print(f\"   ğŸ“‹ Behavioral features: {len(clustering_data['feature_names'])}\")\n",
    "    print(f\"   ğŸŒ Works across: beauty, gaming, food, education, etc.\")\n",
    "    \n",
    "    # Show sample of prepared data with behavioral interpretation\n",
    "    print(f\"\\nğŸ“ Sample behavioral patterns (first 3 comments):\")\n",
    "    sample_data = clustering_data['X_clustering'].head(3)\n",
    "    for i, (idx, row) in enumerate(sample_data.iterrows()):\n",
    "        comment_text = clustering_data['text_data'].iloc[i][:50] + \"...\"\n",
    "        print(f\"\\n   Comment {i+1}: \\\"{comment_text}\\\"\")\n",
    "        print(f\"   Behavioral signals:\")\n",
    "        print(f\"     ğŸ“ Text complexity: char_count={row['char_count']:.2f}, repetition={row['repetition_ratio']:.2f}\")\n",
    "        print(f\"     ğŸ˜Š Emoji pattern: count={row['emoji_count']:.2f}, ratio={row['emoji_ratio']:.2f}\")\n",
    "        print(f\"     ğŸ¤– Bot indicators: generic={row['is_generic']:.2f}, caps={row['caps_ratio']:.2f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ Cannot prepare clustering data - processed data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd3cf4",
   "metadata": {},
   "source": [
    "## ğŸŒ€ Step 7: Primary GMM Clustering with Uncertainty Handling\n",
    "\n",
    "**Purpose**: Apply Gaussian Mixture Models following our two-stage clustering strategy with probabilistic uncertainty handling.\n",
    "\n",
    "**What this section does**:\n",
    "1. **Primary GMM (K=2)**: Separate comments into spam vs quality clusters using content-agnostic features\n",
    "2. **Uncertainty handling**: Flag 40-60% confidence comments as uncertain (manual review needed)\n",
    "3. **Cluster interpretation**: Automatically identify which cluster represents spam based on characteristics\n",
    "4. **Probabilistic validation**: Use multiple clustering quality metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin)\n",
    "5. **Business insights**: Provide actionable results with confidence scores\n",
    "\n",
    "**GMM Configuration**:\n",
    "- **n_components=2**: Only spam vs quality (uncertainty handled probabilistically)\n",
    "- **covariance_type='full'**: Allow flexible cluster shapes for diverse content\n",
    "- **Uncertainty thresholds**: 40-60% confidence flagged for manual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1fe2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gaussian Mixture Model Clustering with Uncertainty Handling\n",
    "if 'clustering_data' in locals():\n",
    "    print(\"ğŸŒ€ Applying GMM Clustering with Uncertainty Handling...\\n\")\n",
    "    \n",
    "    # Extract clustering data\n",
    "    X_clustering = clustering_data['X_clustering']\n",
    "    text_data = clustering_data['text_data']\n",
    "    feature_names = clustering_data['feature_names']\n",
    "    \n",
    "    # Define uncertainty thresholds\n",
    "    UNCERTAINTY_LOWER = 0.40\n",
    "    UNCERTAINTY_UPPER = 0.60\n",
    "    \n",
    "    print(f\"ğŸ¯ Uncertainty thresholds: {UNCERTAINTY_LOWER}-{UNCERTAINTY_UPPER} (will be flagged as uncertain)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. PRIMARY GMM CLUSTERING (K=2: Spam vs Quality)\n",
    "    # ========================================\n",
    "    print(f\"\\nğŸ¯ Stage 1: Primary clustering (Spam vs Quality)...\")\n",
    "    \n",
    "    # Initialize GMM for primary clustering\n",
    "    gmm_primary = GaussianMixture(\n",
    "        n_components=2,\n",
    "        covariance_type='full',    # Allow flexible cluster shapes\n",
    "        init_params='kmeans',      # Initialize with k-means\n",
    "        max_iter=200,             # Sufficient convergence iterations\n",
    "        n_init=10,                # Multiple random initializations for stability\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit primary clustering\n",
    "    start_time = time.time()\n",
    "    primary_labels = gmm_primary.fit_predict(X_clustering)\n",
    "    primary_probabilities = gmm_primary.predict_proba(X_clustering)\n",
    "    primary_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   âœ… Primary clustering completed in {primary_time:.2f} seconds\")\n",
    "    print(f\"   ğŸ“Š Initial cluster distribution:\")\n",
    "    unique, counts = np.unique(primary_labels, return_counts=True)\n",
    "    for cluster, count in zip(unique, counts):\n",
    "        print(f\"      Cluster {cluster}: {count:,} comments ({count/len(primary_labels)*100:.1f}%)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. UNCERTAINTY HANDLING & FINAL ASSIGNMENT\n",
    "    # ========================================\n",
    "    print(f\"\\nğŸ¤” Applying uncertainty handling...\")\n",
    "    \n",
    "    # Get maximum probabilities for each comment\n",
    "    max_probabilities = np.max(primary_probabilities, axis=1)\n",
    "    predicted_clusters = np.argmax(primary_probabilities, axis=1)\n",
    "    \n",
    "    # Apply uncertainty logic\n",
    "    def assign_final_labels_with_uncertainty(probabilities, predicted_clusters, spam_cluster_id):\n",
    "        \"\"\"Assign final labels considering uncertainty thresholds\"\"\"\n",
    "        max_probs = np.max(probabilities, axis=1)\n",
    "        final_labels = []\n",
    "        \n",
    "        for i, (max_prob, cluster) in enumerate(zip(max_probs, predicted_clusters)):\n",
    "            if max_prob > UNCERTAINTY_UPPER:\n",
    "                # Confident assignment\n",
    "                final_labels.append('spam' if cluster == spam_cluster_id else 'quality')\n",
    "            else:\n",
    "                # Uncertain - needs manual review\n",
    "                final_labels.append('uncertain')\n",
    "        \n",
    "        return np.array(final_labels)\n",
    "    \n",
    "    # First, determine which cluster is spam vs quality\n",
    "    cluster_stats = {}\n",
    "    for cluster_id in [0, 1]:\n",
    "        mask = primary_labels == cluster_id\n",
    "        cluster_data = X_clustering[mask]\n",
    "        \n",
    "        # Calculate cluster statistics to identify spam cluster\n",
    "        stats = {\n",
    "            'size': mask.sum(),\n",
    "            'avg_char_count': cluster_data['char_count'].mean(),\n",
    "            'generic_percentage': cluster_data['is_generic'].mean() * 100,\n",
    "            'avg_caps_ratio': cluster_data['caps_ratio'].mean(),\n",
    "            'avg_emoji_ratio': cluster_data['emoji_ratio'].mean(),\n",
    "        }\n",
    "        cluster_stats[cluster_id] = stats\n",
    "    \n",
    "    # Identify spam cluster (higher generic %, lower char count, higher caps ratio)\n",
    "    spam_score_0 = (cluster_stats[0]['generic_percentage'] + \n",
    "                    cluster_stats[0]['avg_caps_ratio'] * 100 - \n",
    "                    cluster_stats[0]['avg_char_count'] / 10)\n",
    "    spam_score_1 = (cluster_stats[1]['generic_percentage'] + \n",
    "                    cluster_stats[1]['avg_caps_ratio'] * 100 - \n",
    "                    cluster_stats[1]['avg_char_count'] / 10)\n",
    "    \n",
    "    spam_cluster_id = 0 if spam_score_0 > spam_score_1 else 1\n",
    "    quality_cluster_id = 1 - spam_cluster_id\n",
    "    \n",
    "    print(f\"   ğŸ” Identified Cluster {spam_cluster_id} as SPAM-LIKE\")\n",
    "    print(f\"   ğŸ” Identified Cluster {quality_cluster_id} as QUALITY-LIKE\")\n",
    "    \n",
    "    # Apply final labeling with uncertainty\n",
    "    final_labels = assign_final_labels_with_uncertainty(\n",
    "        primary_probabilities, predicted_clusters, spam_cluster_id\n",
    "    )\n",
    "    \n",
    "    # Calculate distribution\n",
    "    unique_final, counts_final = np.unique(final_labels, return_counts=True)\n",
    "    total_comments = len(final_labels)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Final distribution with uncertainty handling:\")\n",
    "    for label, count in zip(unique_final, counts_final):\n",
    "        percentage = count / total_comments * 100\n",
    "        print(f\"      {label.title()}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. UNCERTAINTY ANALYSIS\n",
    "    # ========================================\n",
    "    print(f\"\\nğŸ” Analyzing uncertain comments...\")\n",
    "    \n",
    "    uncertain_mask = final_labels == 'uncertain'\n",
    "    if uncertain_mask.sum() > 0:\n",
    "        uncertain_probs = max_probabilities[uncertain_mask]\n",
    "        uncertain_texts = text_data[uncertain_mask]\n",
    "        \n",
    "        print(f\"   ğŸ“Š Uncertainty statistics:\")\n",
    "        print(f\"      ğŸ¤” Uncertain rate: {uncertain_mask.mean()*100:.1f}%\")\n",
    "        print(f\"      ğŸ“ˆ Avg confidence: {uncertain_probs.mean():.3f}\")\n",
    "        print(f\"      ğŸ“‰ Min confidence: {uncertain_probs.min():.3f}\")\n",
    "        print(f\"      ğŸ“Š Max confidence: {uncertain_probs.max():.3f}\")\n",
    "        \n",
    "        # Show examples of uncertain comments\n",
    "        print(f\"\\n   ğŸ“ Sample uncertain comments:\")\n",
    "        uncertain_indices = np.where(uncertain_mask)[0][:3]\n",
    "        for i, idx in enumerate(uncertain_indices):\n",
    "            text = text_data.iloc[idx]\n",
    "            prob = max_probabilities[idx]\n",
    "            cluster = predicted_clusters[idx]\n",
    "            cluster_name = 'spam' if cluster == spam_cluster_id else 'quality'\n",
    "            print(f\"      {i+1}. \\\"{text[:60]}...\\\" \")\n",
    "            print(f\"         â†’ {prob:.3f} confidence toward {cluster_name}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. CLUSTERING QUALITY METRICS\n",
    "    # ========================================\n",
    "    print(f\"\\nğŸ“ˆ Evaluating clustering quality...\")\n",
    "    \n",
    "    # Calculate clustering metrics (using original binary clustering)\n",
    "    silhouette_avg = silhouette_score(X_clustering, primary_labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_clustering, primary_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_clustering, primary_labels)\n",
    "    \n",
    "    print(f\"   ğŸ“Š Silhouette Score: {silhouette_avg:.3f} (higher is better, >0.5 is good)\")\n",
    "    print(f\"   ğŸ“Š Calinski-Harabasz Index: {calinski_harabasz:.1f} (higher is better)\")\n",
    "    print(f\"   ğŸ“Š Davies-Bouldin Index: {davies_bouldin:.3f} (lower is better)\")\n",
    "    \n",
    "    # Model selection metrics\n",
    "    aic = gmm_primary.aic(X_clustering)\n",
    "    bic = gmm_primary.bic(X_clustering)\n",
    "    log_likelihood = gmm_primary.score(X_clustering)\n",
    "    \n",
    "    print(f\"   ğŸ“Š AIC: {aic:.1f} (lower is better)\")\n",
    "    print(f\"   ğŸ“Š BIC: {bic:.1f} (lower is better)\")\n",
    "    print(f\"   ğŸ“Š Log-likelihood: {log_likelihood:.1f} (higher is better)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. CONFIDENT COMMENTS ANALYSIS\n",
    "    # ========================================\n",
    "    print(f\"\\nâœ… Analyzing confident predictions...\")\n",
    "    \n",
    "    # Analyze confident spam and quality comments\n",
    "    confident_spam_mask = final_labels == 'spam'\n",
    "    confident_quality_mask = final_labels == 'quality'\n",
    "    \n",
    "    if confident_spam_mask.sum() > 0:\n",
    "        print(f\"\\n   ğŸš« Confident Spam Examples:\")\n",
    "        spam_indices = np.where(confident_spam_mask)[0][:3]\n",
    "        for i, idx in enumerate(spam_indices):\n",
    "            text = text_data.iloc[idx]\n",
    "            prob = max_probabilities[idx]\n",
    "            print(f\"      {i+1}. \\\"{text}\\\" (confidence: {prob:.3f})\")\n",
    "    \n",
    "    if confident_quality_mask.sum() > 0:\n",
    "        print(f\"\\n   âœ… Confident Quality Examples:\")\n",
    "        quality_indices = np.where(confident_quality_mask)[0][:3]\n",
    "        for i, idx in enumerate(quality_indices):\n",
    "            text = text_data.iloc[idx][:80] + \"...\"\n",
    "            prob = max_probabilities[idx]\n",
    "            print(f\"      {i+1}. \\\"{text}\\\" (confidence: {prob:.3f})\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 6. STORE RESULTS\n",
    "    # ========================================\n",
    "    \n",
    "    # Store results for later use\n",
    "    clustering_results = {\n",
    "        'gmm_primary': gmm_primary,\n",
    "        'primary_labels': primary_labels,\n",
    "        'primary_probabilities': primary_probabilities,\n",
    "        'final_labels': final_labels,\n",
    "        'max_probabilities': max_probabilities,\n",
    "        'spam_cluster_id': spam_cluster_id,\n",
    "        'quality_cluster_id': quality_cluster_id,\n",
    "        'uncertainty_thresholds': {\n",
    "            'lower': UNCERTAINTY_LOWER,\n",
    "            'upper': UNCERTAINTY_UPPER\n",
    "        },\n",
    "        'cluster_stats': cluster_stats,\n",
    "        'metrics': {\n",
    "            'silhouette': silhouette_avg,\n",
    "            'calinski_harabasz': calinski_harabasz,\n",
    "            'davies_bouldin': davies_bouldin,\n",
    "            'aic': aic,\n",
    "            'bic': bic,\n",
    "            'uncertainty_rate': uncertain_mask.mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ… GMM clustering with uncertainty handling completed!\")\n",
    "    print(f\"ğŸ¯ Results summary:\")\n",
    "    print(f\"   ğŸš« Confident spam: {confident_spam_mask.sum():,} ({confident_spam_mask.mean()*100:.1f}%)\")\n",
    "    print(f\"   âœ… Confident quality: {confident_quality_mask.sum():,} ({confident_quality_mask.mean()*100:.1f}%)\")\n",
    "    print(f\"   ğŸ¤” Uncertain (needs review): {uncertain_mask.sum():,} ({uncertain_mask.mean()*100:.1f}%)\")\n",
    "    print(f\"   ğŸ“Š Clustering quality (Silhouette): {silhouette_avg:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot perform clustering - clustering data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8448ef1",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 8: Visualize Clustering Results with Uncertainty Analysis\n",
    "\n",
    "**Purpose**: Create comprehensive visualizations to validate our content-agnostic clustering results and uncertainty handling.\n",
    "\n",
    "**What this section does**:\n",
    "1. **PCA visualization**: Project high-dimensional clusters into 2D space showing spam/quality/uncertain\n",
    "2. **Uncertainty analysis**: Visualize confidence distributions and uncertainty zones\n",
    "3. **Feature importance**: Show which features best separate clusters across content domains\n",
    "4. **Business dashboard**: Provide actionable insights with confidence metrics\n",
    "5. **Cross-domain validation**: Ensure clustering works across beauty, gaming, food, education content\n",
    "\n",
    "**Key Visualizations**:\n",
    "- **Three-color PCA**: Red=Spam, Blue=Quality, Orange=Uncertain\n",
    "- **Confidence histograms**: Distribution by category with uncertainty zone marked\n",
    "- **Uncertainty rate analysis**: Manual review queue sizing\n",
    "- **Feature importance**: Content-agnostic discriminative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Clustering Results with Uncertainty\n",
    "if 'clustering_results' in locals():\n",
    "    print(\"ğŸ“Š Creating clustering visualizations with uncertainty handling...\\n\")\n",
    "    \n",
    "    # Extract data\n",
    "    X_clustering = clustering_data['X_clustering']\n",
    "    primary_labels = clustering_results['primary_labels']\n",
    "    final_labels = clustering_results['final_labels']\n",
    "    max_probabilities = clustering_results['max_probabilities']\n",
    "    spam_cluster_id = clustering_results['spam_cluster_id']\n",
    "    uncertainty_thresholds = clustering_results['uncertainty_thresholds']\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. PCA VISUALIZATION WITH UNCERTAINTY\n",
    "    # ========================================\n",
    "    print(\"ğŸ¯ Creating PCA visualization with uncertainty...\")\n",
    "    \n",
    "    # Apply PCA for 2D visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_clustering)\n",
    "    \n",
    "    # Create enhanced PCA plots\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Plot 1: Three-way classification (spam/quality/uncertain)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    \n",
    "    # Define colors for three categories\n",
    "    color_map = {\n",
    "        'spam': 'red',\n",
    "        'quality': 'blue', \n",
    "        'uncertain': 'orange'\n",
    "    }\n",
    "    colors = [color_map[label] for label in final_labels]\n",
    "    \n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6, s=20)\n",
    "    plt.xlabel(f'PCA Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PCA Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.title('Comments with Uncertainty Handling\\n(Red=Spam, Blue=Quality, Orange=Uncertain)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    import matplotlib.patches as mpatches\n",
    "    red_patch = mpatches.Patch(color='red', label='Confident Spam')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Confident Quality')\n",
    "    orange_patch = mpatches.Patch(color='orange', label='Uncertain')\n",
    "    plt.legend(handles=[red_patch, blue_patch, orange_patch], loc='upper right')\n",
    "    \n",
    "    # Plot 2: Confidence scores with uncertainty zone\n",
    "    plt.subplot(1, 3, 2)\n",
    "    scatter2 = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=max_probabilities, \n",
    "                          cmap='viridis', alpha=0.7, s=20)\n",
    "    plt.xlabel(f'PCA Component 1')\n",
    "    plt.ylabel(f'PCA Component 2')\n",
    "    plt.title('Clustering Confidence Scores\\n(Dark=Low Confidence/Uncertain)')\n",
    "    cbar = plt.colorbar(scatter2, label='Max Probability')\n",
    "    \n",
    "    # Add uncertainty zone markers\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Distribution of confidence scores\n",
    "    plt.subplot(1, 3, 3)\n",
    "    \n",
    "    # Create histogram of confidence scores\n",
    "    plt.hist(max_probabilities, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Add uncertainty zone\n",
    "    plt.axvspan(uncertainty_thresholds['lower'], uncertainty_thresholds['upper'], \n",
    "                alpha=0.3, color='orange', label=f'Uncertainty Zone\\n({uncertainty_thresholds[\"lower\"]}-{uncertainty_thresholds[\"upper\"]})')\n",
    "    \n",
    "    plt.xlabel('Maximum Probability')\n",
    "    plt.ylabel('Number of Comments')\n",
    "    plt.title('Distribution of Clustering Confidence')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   âœ… PCA visualization completed\")\n",
    "    print(f\"   ğŸ“Š PCA explains {pca.explained_variance_ratio_.sum():.1%} of total variance\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. UNCERTAINTY ANALYSIS PLOTS\n",
    "    # ========================================\n",
    "    print(f\"\\nğŸ¤” Creating uncertainty analysis plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Final label distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    final_counts = pd.Series(final_labels).value_counts()\n",
    "    colors_pie = [color_map[label] for label in final_counts.index]\n",
    "    wedges, texts, autotexts = ax1.pie(final_counts.values, labels=final_counts.index, \n",
    "                                      autopct='%1.1f%%', colors=colors_pie)\n",
    "    ax1.set_title('Final Classification Distribution')\n",
    "    \n",
    "    # Plot 2: Confidence score distribution by category\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    for label, color in color_map.items():\n",
    "        mask = final_labels == label\n",
    "        if mask.sum() > 0:\n",
    "            probs = max_probabilities[mask]\n",
    "            ax2.hist(probs, alpha=0.6, label=f'{label.title()} (n={mask.sum():,})', \n",
    "                    color=color, bins=15)\n",
    "    \n",
    "    ax2.axvspan(uncertainty_thresholds['lower'], uncertainty_thresholds['upper'], \n",
    "                alpha=0.2, color='gray', label='Uncertainty Zone')\n",
    "    ax2.set_xlabel('Maximum Probability')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Confidence Distribution by Category')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Feature importance for separation\n",
    "    ax3 = axes[1, 0]\n",
    "    feature_importance = np.abs(pca.components_).mean(axis=0)\n",
    "    top_features_idx = np.argsort(feature_importance)[-8:]  # Top 8 features\n",
    "    top_features = [clustering_data['feature_names'][i] for i in top_features_idx]\n",
    "    top_importance = feature_importance[top_features_idx]\n",
    "    \n",
    "    bars = ax3.barh(range(len(top_features)), top_importance)\n",
    "    ax3.set_yticks(range(len(top_features)))\n",
    "    ax3.set_yticklabels(top_features)\n",
    "    ax3.set_xlabel('PCA Loading (Importance)')\n",
    "    ax3.set_title('Top Features for Clustering')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Uncertainty rate by confidence bins\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Create confidence bins\n",
    "    bins = np.linspace(0.5, 1.0, 11)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    uncertainty_rates = []\n",
    "    \n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (max_probabilities >= bins[i]) & (max_probabilities < bins[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            uncertainty_rate = (final_labels[mask] == 'uncertain').mean()\n",
    "            uncertainty_rates.append(uncertainty_rate)\n",
    "        else:\n",
    "            uncertainty_rates.append(0)\n",
    "    \n",
    "    bars = ax4.bar(bin_centers, uncertainty_rates, width=0.04, alpha=0.7, color='orange')\n",
    "    ax4.set_xlabel('Confidence Score')\n",
    "    ax4.set_ylabel('Uncertainty Rate')\n",
    "    ax4.set_title('Uncertainty Rate by Confidence Level')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add uncertainty threshold lines\n",
    "    ax4.axvline(uncertainty_thresholds['upper'], color='red', linestyle='--', \n",
    "                label=f'Threshold: {uncertainty_thresholds[\"upper\"]}')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. BUSINESS INSIGHTS WITH UNCERTAINTY\n",
    "    # ========================================\n",
    "    print(f\"\\nğŸ’¼ Business Insights with Uncertainty Handling:\")\n",
    "    \n",
    "    total_comments = len(final_labels)\n",
    "    spam_count = (final_labels == 'spam').sum()\n",
    "    quality_count = (final_labels == 'quality').sum()\n",
    "    uncertain_count = (final_labels == 'uncertain').sum()\n",
    "    \n",
    "    print(f\"   ğŸ“Š Comment Classification Results:\")\n",
    "    print(f\"      ğŸš« Confident Spam: {spam_count:,} ({spam_count/total_comments*100:.1f}%)\")\n",
    "    print(f\"      âœ… Confident Quality: {quality_count:,} ({quality_count/total_comments*100:.1f}%)\")\n",
    "    print(f\"      ğŸ¤” Uncertain (Manual Review): {uncertain_count:,} ({uncertain_count/total_comments*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n   ğŸ¯ Uncertainty Analysis:\")\n",
    "    if uncertain_count > 0:\n",
    "        uncertain_probs = max_probabilities[final_labels == 'uncertain']\n",
    "        print(f\"      ğŸ“ˆ Average uncertainty confidence: {uncertain_probs.mean():.3f}\")\n",
    "        print(f\"      ğŸ“Š Uncertainty range: {uncertain_probs.min():.3f} - {uncertain_probs.max():.3f}\")\n",
    "        print(f\"      ğŸ’¡ These {uncertain_count:,} comments need manual review\")\n",
    "    \n",
    "    print(f\"\\n   ğŸ” Model Confidence:\")\n",
    "    confident_mask = final_labels != 'uncertain'\n",
    "    if confident_mask.sum() > 0:\n",
    "        confident_probs = max_probabilities[confident_mask]\n",
    "        print(f\"      âœ… Average confidence for decided comments: {confident_probs.mean():.3f}\")\n",
    "        print(f\"      ğŸ¯ {confident_mask.mean()*100:.1f}% of comments classified with confidence\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    metrics = clustering_results['metrics']\n",
    "    print(f\"\\n   ğŸ“Š Clustering Quality Metrics:\")\n",
    "    print(f\"      ğŸ¯ Silhouette Score: {metrics['silhouette']:.3f}\")\n",
    "    print(f\"      ğŸ“ˆ Uncertainty Rate: {metrics['uncertainty_rate']*100:.1f}%\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\n   ğŸ’¡ Interpretation:\")\n",
    "    if metrics['uncertainty_rate'] < 0.1:\n",
    "        print(f\"      âœ… Low uncertainty rate (<10%) indicates clear cluster separation\")\n",
    "    elif metrics['uncertainty_rate'] < 0.2:\n",
    "        print(f\"      âš ï¸ Moderate uncertainty rate (10-20%) - some ambiguous cases\")\n",
    "    else:\n",
    "        print(f\"      ğŸš¨ High uncertainty rate (>20%) - may need feature engineering review\")\n",
    "    \n",
    "    print(f\"\\nâœ… Uncertainty-aware clustering analysis completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot create visualizations - clustering results not available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
