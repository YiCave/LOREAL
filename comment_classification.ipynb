{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0eaede",
   "metadata": {},
   "source": [
    "# L'Oréal Hackathon: Comment Clustering Strategy Implementation\n",
    "## Unsupervised Spam Detection and Quality Assessment using Gaussian Mixture Models\n",
    "\n",
    "This notebook implements the **comprehensive comment clustering strategy** using **Gaussian Mixture Models (GMM)** to identify spam and assess comment quality across diverse YouTube content. \n",
    "\n",
    "### 🎯 **Clustering Objective**\n",
    "- **Primary Goal**: Use unsupervised clustering to separate YouTube comments into \"Spam\" and \"Quality\" clusters across ALL content types\n",
    "- **Secondary Goal**: Handle uncertainty with probabilistic thresholds (40-60% confidence flagged as uncertain)\n",
    "- **Key Innovation**: No labeled data required - discovers natural patterns in comment characteristics\n",
    "- **Dataset Scope**: Diverse YouTube content - beauty relevance ≠ spam indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90291a44",
   "metadata": {},
   "source": [
    "## 📚 Step 1: Import Required Libraries\n",
    "\n",
    "**Purpose**: Load all necessary libraries for unsupervised clustering, data processing, and visualization following our clustering strategy.\n",
    "\n",
    "**What we're importing**:\n",
    "- **Clustering**: Gaussian Mixture Models from scikit-learn for unsupervised spam detection\n",
    "- **Data handling**: pandas, numpy for processing diverse YouTube content\n",
    "- **Text processing**: NLTK, TextBlob for natural language processing across domains\n",
    "- **Visualization**: matplotlib, plotly for cluster analysis and uncertainty visualization\n",
    "- **Language support**: langdetect for multilingual comment support\n",
    "- **Emoji processing**: emoji library for cross-domain emoji analysis\n",
    "\n",
    "**Key Focus**: Set up environment for content-agnostic spam detection using probabilistic clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ee6d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scikit-learn imported successfully\n",
      "✅ TextBlob imported successfully\n",
      "✅ NLTK imported successfully\n",
      "✅ Langdetect imported successfully\n",
      "✅ Emoji library imported successfully\n",
      "✅ Matplotlib and Seaborn imported successfully\n",
      "✅ Plotly imported successfully\n",
      "\n",
      "🎉 ALL LIBRARIES IMPORTED SUCCESSFULLY!\n",
      "==================================================\n",
      "📊 Pandas version: 2.2.3\n",
      "🤖 Scikit-learn available: True\n",
      "🌀 GMM clustering ready: ✅\n",
      "📝 TextBlob ready: ✅\n",
      "🌍 Langdetect ready: ✅\n",
      "😊 Emoji library ready: ✅\n",
      "📈 Plotly ready: ✅\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "try:\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.decomposition import PCA\n",
    "    print(\"✅ Scikit-learn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Scikit-learn import error: {e}\")\n",
    "\n",
    "# Text processing libraries\n",
    "import re\n",
    "import string\n",
    "\n",
    "# TextBlob\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    print(\"✅ TextBlob imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing textblob...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"textblob\"])\n",
    "    from textblob import TextBlob\n",
    "    print(\"✅ TextBlob installed and imported\")\n",
    "\n",
    "# NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    print(\"✅ NLTK imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing nltk...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "    print(\"✅ NLTK installed and imported\")\n",
    "\n",
    "# Language detection\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0  # For consistent results\n",
    "    print(\"✅ Langdetect imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing langdetect...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"langdetect\"])\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "    print(\"✅ Langdetect installed and imported\")\n",
    "\n",
    "# Emoji processing\n",
    "try:\n",
    "    import emoji\n",
    "    print(\"✅ Emoji library imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing emoji...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"emoji\"])\n",
    "    import emoji\n",
    "    print(\"✅ Emoji library installed and imported\")\n",
    "\n",
    "# Visualization libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"✅ Matplotlib and Seaborn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Matplotlib/Seaborn import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    print(\"✅ Plotly imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing plotly...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"plotly\"])\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    print(\"✅ Plotly installed and imported\")\n",
    "\n",
    "# Utility libraries\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"\\n🎉 ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🤖 Scikit-learn available: {GaussianMixture is not None}\")\n",
    "print(f\"🌀 GMM clustering ready: ✅\")\n",
    "print(f\"📝 TextBlob ready: ✅\")\n",
    "print(f\"🌍 Langdetect ready: ✅\")\n",
    "print(f\"😊 Emoji library ready: ✅\")\n",
    "print(f\"📈 Plotly ready: ✅\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618a687",
   "metadata": {},
   "source": [
    "## 🗂️ Step 2: Data Loading and Initial Exploration\n",
    "\n",
    "**Purpose**: Load the diverse YouTube comment datasets and perform initial exploration following our content-agnostic approach.\n",
    "\n",
    "**What this block does**:\n",
    "1. **Load comment files**: Read all 5 comment CSV files (comments1-5.csv) from diverse content\n",
    "2. **Load video context**: Read videos.csv for cross-domain context information  \n",
    "3. **Data inspection**: Check data shapes, columns, and diversity across content types\n",
    "4. **Memory optimization**: Plan batch processing for large datasets (>50MB each)\n",
    "\n",
    "**Key Philosophy**: \n",
    "- **Content-agnostic approach**: Comments span beauty, lifestyle, entertainment, education, etc.\n",
    "- **Spam definition**: Focus on bot-like behavior, not content relevance\n",
    "- **Preserve diversity**: Maintain all content types for comprehensive clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbf002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading comment data...\n",
      "   📂 Loading comments1.csv...\n",
      "   📂 Loading comments1.csv...\n",
      "   ✅ comments1.csv: 1,000,000 rows loaded\n",
      "   📂 Loading comments2.csv...\n",
      "   ✅ comments1.csv: 1,000,000 rows loaded\n",
      "   📂 Loading comments2.csv...\n",
      "   ✅ comments2.csv: 999,999 rows loaded\n",
      "   📂 Loading comments3.csv...\n",
      "   ✅ comments2.csv: 999,999 rows loaded\n",
      "   📂 Loading comments3.csv...\n",
      "   ✅ comments3.csv: 999,999 rows loaded\n",
      "   📂 Loading comments4.csv...\n",
      "   ✅ comments3.csv: 999,999 rows loaded\n",
      "   📂 Loading comments4.csv...\n",
      "   ✅ comments4.csv: 999,999 rows loaded\n",
      "   📂 Loading comments5.csv...\n",
      "   ✅ comments4.csv: 999,999 rows loaded\n",
      "   📂 Loading comments5.csv...\n",
      "   ✅ comments5.csv: 725,015 rows loaded\n",
      "   ✅ comments5.csv: 725,015 rows loaded\n",
      "\n",
      "✅ All comments combined!\n",
      "📊 Combined shape: (4725012, 10)\n",
      "\n",
      "🎥 Loading video data...\n",
      "\n",
      "✅ All comments combined!\n",
      "📊 Combined shape: (4725012, 10)\n",
      "\n",
      "🎥 Loading video data...\n",
      "✅ Videos loaded!\n",
      "📊 Videos shape: (92759, 15)\n",
      "\n",
      "🎉 DATA LOADING COMPLETE!\n",
      "   💬 Comments: 4,725,012 rows\n",
      "   🎥 Videos: 92,759 rows\n",
      "   🔗 Unique videos in comments: 39,938\n",
      "   ✅ Ready for feature engineering!\n",
      "✅ Videos loaded!\n",
      "📊 Videos shape: (92759, 15)\n",
      "\n",
      "🎉 DATA LOADING COMPLETE!\n",
      "   💬 Comments: 4,725,012 rows\n",
      "   🎥 Videos: 92,759 rows\n",
      "   🔗 Unique videos in comments: 39,938\n",
      "   ✅ Ready for feature engineering!\n"
     ]
    }
   ],
   "source": [
    "# Clear output and load data\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Clear any previous output to prevent duplication display\n",
    "clear_output(wait=True)\n",
    "\n",
    "data_path = r'c:\\Users\\user\\OneDrive\\Documents\\LOREAL HACKATHON\\dataset' \n",
    "comment_files = [f'comments{i}.csv' for i in range(1, 6)]\n",
    "video_file = 'videos.csv'\n",
    "\n",
    "print(\"🔄 Loading comment data...\")\n",
    "\n",
    "# Load all comment files\n",
    "all_comments = []\n",
    "for i, file in enumerate(comment_files, 1):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    print(f\"   📂 Loading {file}...\")\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        low_memory=False,       # prevents chunk guessing\n",
    "        dtype=str,              # read everything as string (fast, safe)\n",
    "    )\n",
    "    all_comments.append(df)\n",
    "    print(f\"   ✅ {file}: {len(df):,} rows loaded\")\n",
    "\n",
    "# Combine all comments\n",
    "comments_df = pd.concat(all_comments, ignore_index=True)\n",
    "print(f\"\\n✅ All comments combined!\")\n",
    "print(f\"📊 Combined shape: {comments_df.shape}\")\n",
    "\n",
    "print(f\"\\n🎥 Loading video data...\")\n",
    "try:\n",
    "    videos_df = pd.read_csv(os.path.join(data_path, video_file))\n",
    "    print(f\"✅ Videos loaded!\")\n",
    "    print(f\"📊 Videos shape: {videos_df.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading videos: {e}\")\n",
    "    videos_df = None\n",
    "\n",
    "print(f\"\\n🎉 DATA LOADING COMPLETE!\")\n",
    "print(f\"   💬 Comments: {comments_df.shape[0]:,} rows\")\n",
    "print(f\"   🎥 Videos: {videos_df.shape[0]:,} rows\")\n",
    "print(f\"   🔗 Unique videos in comments: {comments_df['videoId'].nunique():,}\")\n",
    "print(f\"   ✅ Ready for feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2810e",
   "metadata": {},
   "source": [
    "## 🧹 Step 3: Text Preprocessing Functions\n",
    "\n",
    "**Purpose**: Create utility functions for cleaning and preprocessing text data following our content-agnostic clustering strategy.\n",
    "\n",
    "**What these functions do**:\n",
    "1. **`clean_text()`**: Light cleaning preserving domain-specific content (beauty, gaming, food, etc.)\n",
    "2. **`detect_language_safe()`**: Safely detect comment language across international content\n",
    "3. **`extract_emoji_features()`**: Extract emoji patterns for spam detection across all domains  \n",
    "4. **`calculate_text_stats()`**: Calculate text statistics for bot-like behavior detection\n",
    "\n",
    "**Key Philosophy**:\n",
    "- **Preserve content diversity**: Don't remove domain-specific terminology\n",
    "- **Focus on behavior patterns**: Detect spam through patterns, not content type\n",
    "- **Cross-domain emoji analysis**: Emojis relevant to respective content (🎵 for music, 🍕 for food)\n",
    "- **Language-agnostic approach**: Behavioral patterns work across all languages without translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c19c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing preprocessing functions with multilingual support...\n",
      "\n",
      "Test 1: This is amazing! 😍✨ Love this tutorial 💄👍 Check out my channel: https://example.com\n",
      "   🌍 Language: en\n",
      "   🔄 Translated: This is amazing! 😍✨ Love this tutorial 💄👍 Check out my channel: https://example.com\n",
      "   ✅ Translation attempted: False\n",
      "   📝 Cleaned: This is amazing! 😍✨ Love this tutorial 💄👍 Check out my channel: https://example.com\n",
      "   😊 Emojis: 4 total, sentiment: 3.1\n",
      "   📊 Stats: 83 chars, 13 words\n",
      "\n",
      "Test 2: ¡Esto es increíble! Me encanta este tutorial 💄\n",
      "   🌍 Language: unknown\n",
      "   🔄 Translated: ¡Esto es increíble! Me encanta este tutorial 💄\n",
      "   ✅ Translation attempted: False\n",
      "   📝 Cleaned: ¡Esto es increíble! Me encanta este tutorial 💄\n",
      "   😊 Emojis: 1 total, sentiment: 0.1\n",
      "   📊 Stats: 46 chars, 8 words\n",
      "\n",
      "Test 3: C'est magnifique! J'adore ce tutoriel beauté 💋\n",
      "   🌍 Language: unknown\n",
      "   🔄 Translated: C'est magnifique! J'adore ce tutoriel beauté 💋\n",
      "   ✅ Translation attempted: False\n",
      "   📝 Cleaned: C'est magnifique! J'adore ce tutoriel beauté 💋\n",
      "   😊 Emojis: 1 total, sentiment: 0.1\n",
      "   📊 Stats: 46 chars, 7 words\n",
      "\n",
      "Test 4: 素晴らしい！このチュートリアルが大好きです ✨\n",
      "   🌍 Language: unknown\n",
      "   🔄 Translated: 素晴らしい！このチュートリアルが大好きです ✨\n",
      "   ✅ Translation attempted: False\n",
      "   📝 Cleaned: 素晴らしい！このチュートリアルが大好きです ✨\n",
      "   😊 Emojis: 1 total, sentiment: 1.0\n",
      "   📊 Stats: 23 chars, 2 words\n",
      "\n",
      "Test 5: مذهل! أحب هذا الفيديو التعليمي 💕\n",
      "   🌍 Language: unknown\n",
      "   🔄 Translated: مذهل! أحب هذا الفيديو التعليمي 💕\n",
      "   ✅ Translation attempted: False\n",
      "   📝 Cleaned: مذهل! أحب هذا الفيديو التعليمي 💕\n",
      "   😊 Emojis: 1 total, sentiment: 1.0\n",
      "   📊 Stats: 32 chars, 6 words\n",
      "\n",
      "\n",
      "✅ All preprocessing functions with translation created successfully!\n",
      "\n",
      "🌍 Translation Features:\n",
      "   🔄 Auto-translate non-English comments to English\n",
      "   🌏 Preserve original text for emoji analysis\n",
      "   📊 Enable content-agnostic spam detection across languages\n",
      "   🎯 Maintain cultural context while standardizing text analysis\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text while preserving meaningful content.\n",
    "    \n",
    "    This function performs light cleaning to standardize text without\n",
    "    removing important information like emojis or context.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string and handle encoding issues\n",
    "    text = str(text)\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove excessive punctuation (more than 3 consecutive)\n",
    "    text = re.sub(r'[!]{4,}', '!!!', text)\n",
    "    text = re.sub(r'[?]{4,}', '???', text)\n",
    "    text = re.sub(r'[.]{4,}', '...', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def detect_language_safe(text, confidence_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Safely detect language with confidence scoring.\n",
    "    \n",
    "    Returns language code if detection confidence is high enough,\n",
    "    otherwise returns 'unknown'. Used for metadata only.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(str(text).strip()) < 3:  # Too short to detect\n",
    "            return 'unknown'\n",
    "        \n",
    "        # Remove emojis for language detection\n",
    "        text_no_emoji = emoji.demojize(str(text))\n",
    "        \n",
    "        detected_lang = detect(text_no_emoji)\n",
    "        return detected_lang\n",
    "    \n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def calculate_text_stats(text):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive text statistics.\n",
    "    \n",
    "    Returns various text metrics useful for spam detection.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Basic stats\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    \n",
    "    # Advanced stats\n",
    "    avg_word_length = np.mean([len(word) for word in text.split()]) if word_count > 0 else 0\n",
    "    \n",
    "    # Capitalization analysis\n",
    "    upper_count = sum(1 for c in text if c.isupper())\n",
    "    caps_ratio = upper_count / max(char_count, 1)\n",
    "    \n",
    "    # Special character analysis\n",
    "    special_chars = sum(1 for c in text if c in string.punctuation)\n",
    "    special_ratio = special_chars / max(char_count, 1)\n",
    "    \n",
    "    # URL detection\n",
    "    url_count = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "    \n",
    "    # Repetition detection (same word repeated)\n",
    "    words = text.lower().split()\n",
    "    word_freq = Counter(words)\n",
    "    max_word_freq = max(word_freq.values()) if words else 0\n",
    "    repetition_ratio = max_word_freq / max(word_count, 1)\n",
    "    \n",
    "    return {\n",
    "        'char_count': char_count,\n",
    "        'word_count': word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'special_ratio': special_ratio,\n",
    "        'url_count': url_count,\n",
    "        'repetition_ratio': repetition_ratio,\n",
    "        'has_url': 1 if url_count > 0 else 0\n",
    "    }\n",
    "\n",
    "def extract_emoji_features(text):\n",
    "    \"\"\"\n",
    "    Extract comprehensive emoji features for spam detection.\n",
    "    \n",
    "    Returns dictionary with emoji statistics and spam indicators.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Extract all emojis\n",
    "    emojis = emoji.emoji_list(text)\n",
    "    emoji_chars = [item['emoji'] for item in emojis]\n",
    "    \n",
    "    # Basic emoji stats\n",
    "    emoji_count = len(emoji_chars)\n",
    "    unique_emojis = len(set(emoji_chars))\n",
    "    text_length = len(text)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    emoji_ratio = emoji_count / max(text_length, 1)\n",
    "    emoji_diversity = unique_emojis / max(emoji_count, 1)\n",
    "    \n",
    "    # EXPANDED sentiment emojis (broader coverage)\n",
    "    positive_emojis = ['😍', '💕', '❤️', '✨', '🌟', '😊', '👍', '🔥', '💯', '🥰', '😘',\n",
    "                       '🤩', '😎', '🥳', '🤗', '😇', '🙌', '👏', '💪', '🎉', '🙏', '☺️', \n",
    "                       '😀', '😁', '😂', '🤣', '😃', '😄', '😆', '🥲', '💖', '💗', '💝']\n",
    "    \n",
    "    negative_emojis = ['😒', '😕', '👎', '😞', '😠', '😡', '💔', '😢', '😭', '🙄', \n",
    "                       '😤', '😩', '🤮', '🤢', '😵', '🥺', '😖', '😓', '😰', '😨',\n",
    "                       '😱', '🤬', '😈', '💀', '😷', '🤒', '🤕']\n",
    "    \n",
    "    # Count sentiment emojis\n",
    "    positive_count = sum([emoji_chars.count(e) for e in positive_emojis])\n",
    "    negative_count = sum([emoji_chars.count(e) for e in negative_emojis])\n",
    "    \n",
    "    # Count unknown/neutral emojis (not in positive or negative lists)\n",
    "    known_emojis = set(positive_emojis + negative_emojis)\n",
    "    unknown_emoji_count = len([e for e in emoji_chars if e not in known_emojis])\n",
    "    \n",
    "    # Content category emojis\n",
    "    music_emojis = ['🎵', '🎤', '🎸', '🎼', '🎶', '🎧', '🎹', '🥁', '🎺', '🎻']\n",
    "    food_emojis = ['🍕', '🍔', '🥗', '🍰', '🍜', '🍎', '🍌', '🥑', '🍓', '🍉', '🧁']\n",
    "    beauty_emojis = ['💄', '💋', '👄', '💅', '🧴', '🪞', '✨', '💎', '👗', '👠']\n",
    "    \n",
    "    music_count = sum([emoji_chars.count(e) for e in music_emojis])\n",
    "    food_count = sum([emoji_chars.count(e) for e in food_emojis])\n",
    "    beauty_count = sum([emoji_chars.count(e) for e in beauty_emojis])\n",
    "    \n",
    "    # Enhanced sentiment scoring (handles unknown emojis)\n",
    "    # Give small positive weight to unknown emojis (engagement indicator)\n",
    "    sentiment_score = positive_count - negative_count + (unknown_emoji_count * 0.1)\n",
    "    \n",
    "    return {\n",
    "        'emoji_count': emoji_count,\n",
    "        'emoji_ratio': emoji_ratio,\n",
    "        'emoji_diversity': emoji_diversity,\n",
    "        'positive_emoji_count': positive_count,\n",
    "        'negative_emoji_count': negative_count,\n",
    "        'unknown_emoji_count': unknown_emoji_count,\n",
    "        'emoji_sentiment_score': sentiment_score,\n",
    "        'music_emoji_count': music_count,\n",
    "        'food_emoji_count': food_count,\n",
    "        'beauty_emoji_count': beauty_count,\n",
    "        'spam_emoji_indicator': 1 if emoji_ratio > 0.3 and emoji_diversity < 0.5 else 0\n",
    "    }\n",
    "\n",
    "# Test the functions with sample comments (no translation needed)\n",
    "test_comments = [\n",
    "    \"This is amazing! 😍✨ Love this tutorial 💄👍 Check out my channel: https://example.com\",\n",
    "    \"¡Esto es increíble! Me encanta este tutorial 💄\",\n",
    "    \"C'est magnifique! J'adore ce tutoriel beauté 💋\",\n",
    "    \"素晴らしい！このチュートリアルが大好きです ✨\",\n",
    "    \"مذهل! أحب هذا الفيديو التعليمي 💕\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing preprocessing functions (language-agnostic approach)...\\n\")\n",
    "\n",
    "for i, comment in enumerate(test_comments, 1):\n",
    "    print(f\"Test {i}: {comment}\")\n",
    "    \n",
    "    # Test language detection (metadata only)\n",
    "    detected_lang = detect_language_safe(comment)\n",
    "    print(f\"   🌍 Language: {detected_lang}\")\n",
    "    \n",
    "    # Test other functions on original text\n",
    "    cleaned = clean_text(comment)\n",
    "    emoji_features = extract_emoji_features(comment)\n",
    "    text_stats = calculate_text_stats(comment)\n",
    "    \n",
    "    print(f\"   📝 Cleaned: {cleaned}\")\n",
    "    print(f\"   😊 Emojis: {emoji_features['emoji_count']} total, sentiment: {emoji_features['emoji_sentiment_score']}\")\n",
    "    print(f\"   📊 Stats: {text_stats['char_count']} chars, {text_stats['word_count']} words\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n✅ All preprocessing functions created successfully!\")\n",
    "print(\"\\n\udfaf Language-Agnostic Features:\")\n",
    "print(\"   \udcdd Text: Bot-like patterns work across all languages\")\n",
    "print(\"   😊 Emoji: Universal emotional expression patterns\") \n",
    "print(\"   📊 Engagement: Behavioral patterns are language-independent\")\n",
    "print(\"   \udf0d Language: Detected for metadata only (no translation needed)\")\n",
    "print(\"   🚀 Result: Fast, effective spam detection without translation overhead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98b375",
   "metadata": {},
   "source": [
    "## 🔧 Step 4: Feature Engineering Pipeline\n",
    "\n",
    "**Purpose**: Create a comprehensive feature engineering pipeline following our content-agnostic clustering strategy.\n",
    "\n",
    "**What this pipeline does**:\n",
    "1. **Text complexity analysis**: Extract patterns that differentiate spam from genuine engagement\n",
    "2. **Cross-domain emoji analysis**: Process emojis contextually (music 🎵, food 🍕, beauty 💄)\n",
    "3. **Engagement authenticity**: Identify suspicious engagement patterns across all content types\n",
    "4. **Behavioral indicators**: Focus on bot-like patterns rather than content relevance\n",
    "5. **Context preservation**: Maintain video context for relevance assessment\n",
    "\n",
    "**Key Features for GMM Clustering**:\n",
    "- **Text-based**: Complexity, repetition patterns, generic responses\n",
    "- **Emoji & Symbol**: Sentiment, domain-relevance, spam patterns  \n",
    "- **Engagement**: Authenticity indicators, suspicious patterns\n",
    "- **Context Relevance**: General video context matching (not domain-specific)\n",
    "- **Language & Cultural**: Cross-cultural engagement patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ee9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_pipeline(comments_df, videos_df=None):\n",
    "    \"\"\"\n",
    "    Content-agnostic feature engineering pipeline for GMM clustering.\n",
    "    \n",
    "    This function processes comments from diverse domains (beauty, gaming, food, education)\n",
    "    and languages, extracting behavioral patterns for spam detection without translation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🔧 Processing {len(comments_df):,} comments with language-agnostic approach...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize feature dataframe\n",
    "    features_df = comments_df.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. BEHAVIORAL TEXT PROCESSING (ORIGINAL TEXT)\n",
    "    # ========================================\n",
    "    print(\"   📝 Processing behavioral text features...\")\n",
    "    \n",
    "    # Light cleaning preserving domain-specific content\n",
    "    features_df['cleaned_text'] = features_df['textOriginal'].apply(clean_text)\n",
    "    \n",
    "    # Language detection for metadata only\n",
    "    features_df['original_language'] = features_df['textOriginal'].apply(detect_language_safe)\n",
    "    features_df['is_english'] = (features_df['original_language'] == 'en').astype(int)\n",
    "    \n",
    "    # Language diversity features (metadata only)\n",
    "    features_df['is_major_language'] = features_df['original_language'].isin(\n",
    "        ['en', 'es', 'fr', 'de', 'it', 'pt', 'ja', 'ko', 'zh', 'ar', 'hi', 'ru']\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"      📊 Languages detected: {features_df['original_language'].nunique()}\")\n",
    "    \n",
    "    # Text statistics for bot detection (works on original text)\n",
    "    text_stats = features_df['cleaned_text'].apply(calculate_text_stats)\n",
    "    text_stats_df = pd.DataFrame(text_stats.tolist())\n",
    "    features_df = pd.concat([features_df, text_stats_df], axis=1)\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. CROSS-DOMAIN EMOJI ANALYSIS\n",
    "    # ========================================\n",
    "    print(\"   😊 Processing cross-domain emoji features...\")\n",
    "    \n",
    "    emoji_features = features_df['textOriginal'].apply(extract_emoji_features)\n",
    "    emoji_features_df = pd.DataFrame(emoji_features.tolist())\n",
    "    features_df = pd.concat([features_df, emoji_features_df], axis=1)\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. ENGAGEMENT AUTHENTICITY (CONTENT-AGNOSTIC)\n",
    "    # ========================================\n",
    "    print(\"   📊 Processing engagement authenticity features...\")\n",
    "    \n",
    "    # Basic engagement metrics\n",
    "    features_df['likeCount'] = pd.to_numeric(features_df['likeCount'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Content-agnostic engagement patterns\n",
    "    if videos_df is not None:\n",
    "        # Merge with video data for context (any content type)\n",
    "        video_engagement = videos_df.groupby('videoId').agg({\n",
    "            'viewCount': 'first',\n",
    "            'likeCount': 'first',\n",
    "            'commentCount': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        video_engagement = video_engagement.rename(columns={'likeCount': 'video_likeCount'})\n",
    "        features_df = features_df.merge(video_engagement, on='videoId', how='left')\n",
    "        \n",
    "        # Calculate universal engagement ratios\n",
    "        features_df['comment_to_video_like_ratio'] = features_df['likeCount'] / (features_df['video_likeCount'] + 1)\n",
    "        features_df['likes_per_char'] = features_df['likeCount'] / (features_df['char_count'] + 1)\n",
    "    else:\n",
    "        features_df['likes_per_char'] = features_df['likeCount'] / (features_df['char_count'] + 1)\n",
    "    \n",
    "    # Reply structure (universal pattern)\n",
    "    features_df['is_reply'] = (~features_df['parentCommentId'].isna()).astype(int)\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. TEMPORAL PATTERNS (UNIVERSAL)\n",
    "    # ========================================\n",
    "    print(\"   🕐 Processing temporal patterns...\")\n",
    "    \n",
    "    # Convert dates\n",
    "    features_df['publishedAt'] = pd.to_datetime(features_df['publishedAt'], errors='coerce')\n",
    "    \n",
    "    # Extract universal time features\n",
    "    features_df['hour_of_day'] = features_df['publishedAt'].dt.hour\n",
    "    features_df['day_of_week'] = features_df['publishedAt'].dt.dayofweek\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. LANGUAGE-AGNOSTIC SPAM BEHAVIOR DETECTION\n",
    "    # ========================================\n",
    "    print(\"   🚫 Processing language-agnostic spam behavior indicators...\")\n",
    "    \n",
    "    # Generic comment detection (works on original text with regex)\n",
    "    generic_patterns = [\n",
    "        r'^(first|1st)!?$',\n",
    "        r'^(nice|good|great|awesome|amazing|cool)!*$',\n",
    "        r'^(love it|love this|loved it)!*$',\n",
    "        r'^(thanks|thank you)!*$',\n",
    "        r'^(wow|omg|lol|haha)!*$'\n",
    "    ]\n",
    "    \n",
    "    features_df['is_generic'] = 0\n",
    "    for pattern in generic_patterns:\n",
    "        mask = features_df['cleaned_text'].str.lower().str.match(pattern, na=False)\n",
    "        features_df.loc[mask, 'is_generic'] = 1\n",
    "    \n",
    "    # Suspicious engagement patterns (universal)\n",
    "    features_df['suspicious_engagement'] = (\n",
    "        (features_df['char_count'] < 10) & \n",
    "        (features_df['likeCount'] > 5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Bot-like behavior indicators\n",
    "    features_df['excessive_caps'] = (features_df['caps_ratio'] > 0.5).astype(int)\n",
    "    features_df['excessive_repetition'] = (features_df['repetition_ratio'] > 0.7).astype(int)\n",
    "    \n",
    "    # Short non-English spam indicator (behavioral, not content-based)\n",
    "    features_df['short_non_english_spam'] = (\n",
    "        (features_df['is_english'] == 0) & \n",
    "        (features_df['char_count'] < 15) & \n",
    "        (features_df['emoji_count'] == 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # ========================================\n",
    "    # 6. QUALITY INDICATORS (UNIVERSAL)\n",
    "    # ========================================\n",
    "    print(\"   ⭐ Processing universal quality indicators...\")\n",
    "    \n",
    "    # Minimum viable engagement\n",
    "    features_df['sufficient_length'] = (features_df['char_count'] >= 10).astype(int)\n",
    "    \n",
    "    # Balanced communication patterns\n",
    "    features_df['balanced_punctuation'] = (\n",
    "        (features_df['special_ratio'] > 0.01) & \n",
    "        (features_df['special_ratio'] < 0.3)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Meaningful emoji usage (cross-domain)\n",
    "    features_df['meaningful_emoji_usage'] = (\n",
    "        (features_df['emoji_count'] > 0) & \n",
    "        (features_df['emoji_ratio'] < 0.3) & \n",
    "        (features_df['emoji_diversity'] > 0.5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Authentic engagement indicator\n",
    "    features_df['authentic_engagement'] = (\n",
    "        (features_df['char_count'] >= 15) &\n",
    "        (features_df['caps_ratio'] < 0.4) &\n",
    "        (features_df['repetition_ratio'] < 0.5) &\n",
    "        (features_df['special_ratio'] < 0.25)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Cross-cultural engagement (non-English but substantial)\n",
    "    features_df['substantial_non_english'] = (\n",
    "        (features_df['is_english'] == 0) & \n",
    "        (features_df['char_count'] >= 20) & \n",
    "        (features_df['emoji_count'] > 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # ========================================\n",
    "    # 7. FINAL FEATURE SELECTION FOR GMM\n",
    "    # ========================================\n",
    "    \n",
    "    # Content-agnostic features for clustering (behavioral + language metadata)\n",
    "    feature_columns = [\n",
    "        # Behavioral text features (language-agnostic)\n",
    "        'char_count', 'word_count', 'avg_word_length',\n",
    "        'caps_ratio', 'special_ratio', 'repetition_ratio',\n",
    "        \n",
    "        # Cross-domain emoji features\n",
    "        'emoji_count', 'emoji_ratio', 'emoji_diversity',\n",
    "        'emoji_sentiment_score', 'spam_emoji_indicator',\n",
    "        'beauty_emoji_count', 'music_emoji_count', 'food_emoji_count',\n",
    "        \n",
    "        # Universal engagement features\n",
    "        'likeCount', 'likes_per_char', 'is_reply',\n",
    "        \n",
    "        # Temporal patterns\n",
    "        'hour_of_day', 'day_of_week',\n",
    "        \n",
    "        # Language-agnostic spam behavior indicators\n",
    "        'is_generic', 'suspicious_engagement', 'excessive_caps',\n",
    "        'excessive_repetition', 'has_url', 'url_count',\n",
    "        'short_non_english_spam',\n",
    "        \n",
    "        # Language metadata (not translated content)\n",
    "        'is_english', 'is_major_language', 'substantial_non_english',\n",
    "        \n",
    "        # Universal quality indicators\n",
    "        'sufficient_length', 'balanced_punctuation', \n",
    "        'meaningful_emoji_usage', 'authentic_engagement'\n",
    "    ]\n",
    "    \n",
    "    # Add video context features if available\n",
    "    if 'comment_to_video_like_ratio' in features_df.columns:\n",
    "        feature_columns.append('comment_to_video_like_ratio')\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in feature_columns:\n",
    "        if col in features_df.columns:\n",
    "            features_df[col] = features_df[col].fillna(0)\n",
    "    \n",
    "    # Create final feature matrix\n",
    "    available_features = [col for col in feature_columns if col in features_df.columns]\n",
    "    X = features_df[available_features].copy()\n",
    "    \n",
    "    # Use cleaned original text for analysis\n",
    "    processed_data = {\n",
    "        'features': X,\n",
    "        'text': features_df['cleaned_text'],  # Use cleaned original text\n",
    "        'original_data': features_df,\n",
    "        'feature_names': available_features\n",
    "    }\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"   ✅ Language-agnostic feature engineering completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"   📊 Generated {len(available_features)} behavioral features\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "print(\"✅ Language-agnostic feature engineering pipeline created!\")\n",
    "print(\"\\n🎯 Enhanced Features WITHOUT Translation:\")\n",
    "print(\"   📝 Text: Bot-like patterns analyzed on original text\")\n",
    "print(\"   😊 Emoji: Cross-cultural emoji analysis on original text\")\n",
    "print(\"   📊 Engagement: Universal authenticity indicators\")\n",
    "print(\"   \udf0d Language: Detection for metadata only (no translation)\")\n",
    "print(\"   🕐 Temporal: Universal posting patterns\")\n",
    "print(\"   🚫 Spam: Language-agnostic behavioral patterns\")\n",
    "print(\"   ⭐ Quality: Cross-cultural meaningful engagement patterns\")\n",
    "print(\"   🚀 Result: Fast, effective, language-agnostic spam detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4cd94",
   "metadata": {},
   "source": [
    "## 📂 Step 5: Load and Process Sample Data\n",
    "\n",
    "**Purpose**: Load a sample of diverse comment data to test our content-agnostic feature engineering pipeline.\n",
    "\n",
    "**What this block does**:\n",
    "1. **Sample loading**: Load first 10,000 comments from comments1.csv representing diverse content\n",
    "2. **Pipeline testing**: Run our cross-domain feature engineering pipeline on real data\n",
    "3. **Diversity analysis**: Examine features across different content types (beauty, gaming, food, etc.)\n",
    "4. **Quality check**: Verify that features capture behavioral patterns rather than content bias\n",
    "\n",
    "**Why start with a sample**: Testing with diverse content first helps us ensure our clustering approach works across all domains before processing the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data for testing\n",
    "print(\"🔄 Loading sample comment data...\")\n",
    "\n",
    "try:\n",
    "    # Load first batch of comments (sample size for testing)\n",
    "    sample_size = 10000\n",
    "    comments_sample = pd.read_csv(\n",
    "        os.path.join(data_path, 'comments1.csv'), \n",
    "        nrows=sample_size\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Loaded {len(comments_sample):,} sample comments\")\n",
    "    print(f\"📋 Columns: {list(comments_sample.columns)}\")\n",
    "    \n",
    "    # Display basic info about the sample\n",
    "    print(f\"\\n📊 Sample Data Overview:\")\n",
    "    print(f\"   💬 Comments: {len(comments_sample):,}\")\n",
    "    print(f\"   🎥 Unique videos: {comments_sample['videoId'].nunique():,}\")\n",
    "    print(f\"   👥 Unique authors: {comments_sample['authorId'].nunique():,}\")\n",
    "    print(f\"   💝 Total likes: {comments_sample['likeCount'].sum():,}\")\n",
    "    \n",
    "    # Show sample comments\n",
    "    print(f\"\\n📝 Sample Comments:\")\n",
    "    for i in range(3):\n",
    "        comment = comments_sample['textOriginal'].iloc[i]\n",
    "        likes = comments_sample['likeCount'].iloc[i]\n",
    "        print(f\"   {i+1}. \\\"{comment[:100]}...\\\" (❤️ {likes} likes)\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\n🔍 Missing Values:\")\n",
    "    missing_counts = comments_sample.isnull().sum()\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"   {col}: {count:,} ({count/len(comments_sample)*100:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading sample data: {e}\")\n",
    "    comments_sample = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b42edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature engineering pipeline\n",
    "if comments_sample is not None:\n",
    "    print(\"🔧 Testing feature engineering pipeline...\\n\")\n",
    "    \n",
    "    # Process sample data through our pipeline\n",
    "    processed_data = create_feature_pipeline(comments_sample, videos_df)\n",
    "    \n",
    "    # Extract results\n",
    "    X_features = processed_data['features']\n",
    "    text_data = processed_data['text']\n",
    "    feature_names = processed_data['feature_names']\n",
    "    \n",
    "    print(f\"\\n📊 Feature Engineering Results:\")\n",
    "    print(f\"   📈 Feature matrix shape: {X_features.shape}\")\n",
    "    print(f\"   📋 Features generated: {len(feature_names)}\")\n",
    "    \n",
    "    # Display feature statistics\n",
    "    print(f\"\\n📈 Feature Statistics:\")\n",
    "    feature_stats = X_features.describe()\n",
    "    print(feature_stats.round(3))\n",
    "    \n",
    "    # Check for any issues\n",
    "    print(f\"\\n🔍 Data Quality Check:\")\n",
    "    print(f\"   ❓ Missing values: {X_features.isnull().sum().sum()}\")\n",
    "    print(f\"   ♾️ Infinite values: {np.isinf(X_features).sum().sum()}\")\n",
    "    print(f\"   📊 All numeric: {X_features.dtypes.apply(lambda x: np.issubdtype(x, np.number)).all()}\")\n",
    "    \n",
    "    # Show sample of features\n",
    "    print(f\"\\n📝 Sample Feature Values (first 3 comments):\")\n",
    "    sample_features = X_features.head(3)\n",
    "    for i, (idx, row) in enumerate(sample_features.iterrows()):\n",
    "        print(f\"\\n   Comment {i+1}: \\\"{text_data.iloc[i][:60]}...\\\"\")\n",
    "        print(f\"   Features: char_count={row['char_count']}, emoji_count={row['emoji_count']}, \")\n",
    "        print(f\"            likeCount={row['likeCount']}, is_generic={row['is_generic']}\")\n",
    "    \n",
    "    print(f\"\\n✅ Pipeline testing completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot test pipeline - sample data not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5973262",
   "metadata": {},
   "source": [
    "## 🌀 Step 6: Prepare Data for GMM Clustering with Uncertainty Handling\n",
    "\n",
    "**Purpose**: Prepare our engineered features for Gaussian Mixture Model clustering following our probabilistic uncertainty strategy.\n",
    "\n",
    "**What this approach does**:\n",
    "1. **Feature scaling**: Normalize features for optimal GMM performance across all domains\n",
    "2. **Feature selection**: Choose most discriminative features for content-agnostic spam detection\n",
    "3. **Uncertainty framework**: Prepare for 3-way classification (spam/quality/uncertain)\n",
    "4. **Cross-domain validation**: Ensure features work across beauty, gaming, food, education content\n",
    "5. **Data quality checks**: Validate data for unsupervised learning\n",
    "\n",
    "**Uncertainty Thresholds**:\n",
    "- **Confident assignments**: >60% probability \n",
    "- **Uncertain zone**: 40-60% probability (flagged for manual review)\n",
    "- **Business value**: Avoids forced classifications of genuinely ambiguous comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc00697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_clustering_data(processed_data, scale_features=True):\n",
    "    \"\"\"\n",
    "    Prepare data for GMM clustering following content-agnostic strategy.\n",
    "    \n",
    "    This function prepares behavioral features for unsupervised clustering\n",
    "    to identify spam patterns across diverse content domains.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_features = processed_data['features']\n",
    "    text_data = processed_data['text']\n",
    "    original_data = processed_data['original_data']\n",
    "    \n",
    "    print(\"🌀 Preparing data for content-agnostic GMM clustering...\\n\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. CONTENT-AGNOSTIC FEATURE SELECTION\n",
    "    # ========================================\n",
    "    \n",
    "    # Select features that work across all content domains\n",
    "    clustering_features = [\n",
    "        # Behavioral text complexity (universal)\n",
    "        'char_count', 'word_count', 'avg_word_length',\n",
    "        \n",
    "        # Bot-like behavior indicators (universal)\n",
    "        'caps_ratio', 'special_ratio', 'repetition_ratio',\n",
    "        'is_generic', 'excessive_caps', 'excessive_repetition', 'url_count',\n",
    "        \n",
    "        # Cross-domain emoji patterns\n",
    "        'emoji_count', 'emoji_ratio', 'emoji_diversity',\n",
    "        'emoji_sentiment_score', 'spam_emoji_indicator',\n",
    "        \n",
    "        # Universal engagement authenticity\n",
    "        'likeCount', 'likes_per_char', 'is_reply',\n",
    "        'suspicious_engagement', 'authentic_engagement',\n",
    "        \n",
    "        # Quality indicators (work across domains)\n",
    "        'sufficient_length', 'balanced_punctuation', 'meaningful_emoji_usage'\n",
    "    ]\n",
    "    \n",
    "    # Add video context features if available\n",
    "    if 'comment_to_video_like_ratio' in X_features.columns:\n",
    "        clustering_features.append('comment_to_video_like_ratio')\n",
    "    \n",
    "    # Select available features\n",
    "    available_features = [f for f in clustering_features if f in X_features.columns]\n",
    "    X_clustering = X_features[available_features].copy()\n",
    "    \n",
    "    print(f\"📊 Selected {len(available_features)} content-agnostic features:\")\n",
    "    print(\"   🎯 Behavioral patterns that work across beauty, gaming, food, education:\")\n",
    "    for i, feature in enumerate(available_features):\n",
    "        category = \"🤖 Bot-like\" if feature in ['caps_ratio', 'repetition_ratio', 'is_generic'] else \\\n",
    "                   \"😊 Emoji\" if 'emoji' in feature else \\\n",
    "                   \"📊 Engagement\" if feature in ['likeCount', 'likes_per_char'] else \\\n",
    "                   \"⭐ Quality\" if feature in ['sufficient_length', 'balanced_punctuation'] else \\\n",
    "                   \"📝 Text\"\n",
    "        print(f\"   {i+1:2d}. {feature:<25} ({category})\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. DATA QUALITY CHECKS\n",
    "    # ========================================\n",
    "    \n",
    "    print(f\"\\n🔍 Data quality checks for clustering:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = X_clustering.isnull().sum().sum()\n",
    "    print(f\"   ❓ Missing values: {missing_values}\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    infinite_values = np.isinf(X_clustering).sum().sum()\n",
    "    print(f\"   ♾️ Infinite values: {infinite_values}\")\n",
    "    \n",
    "    # Check feature variance (important for GMM)\n",
    "    zero_variance_features = X_clustering.columns[X_clustering.var() == 0].tolist()\n",
    "    if zero_variance_features:\n",
    "        print(f\"   ⚠️ Zero variance features: {zero_variance_features}\")\n",
    "    \n",
    "    # Fill any remaining missing values\n",
    "    if missing_values > 0:\n",
    "        X_clustering = X_clustering.fillna(0)\n",
    "        print(f\"   ✅ Filled missing values with 0\")\n",
    "    \n",
    "    # Replace infinite values\n",
    "    if infinite_values > 0:\n",
    "        X_clustering = X_clustering.replace([np.inf, -np.inf], [1e6, -1e6])\n",
    "        print(f\"   ✅ Replaced infinite values\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. FEATURE SCALING FOR GMM\n",
    "    # ========================================\n",
    "    \n",
    "    if scale_features:\n",
    "        print(f\"\\n📏 Scaling features for optimal GMM performance...\")\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_clustering)\n",
    "        X_clustering_final = pd.DataFrame(X_scaled, columns=available_features, index=X_clustering.index)\n",
    "        \n",
    "        print(f\"   ✅ Features standardized (mean=0, std=1)\")\n",
    "        \n",
    "        # Show scaling statistics\n",
    "        print(f\"   📊 Scaled feature statistics:\")\n",
    "        print(f\"      📈 Range: [{X_clustering_final.min().min():.3f}, {X_clustering_final.max().max():.3f}]\")\n",
    "        print(f\"      📊 Mean: {X_clustering_final.mean().mean():.3f}\")\n",
    "        print(f\"      📏 Std: {X_clustering_final.std().mean():.3f}\")\n",
    "        \n",
    "    else:\n",
    "        scaler = None\n",
    "        X_clustering_final = X_clustering\n",
    "        print(f\"\\n📏 Using raw features (no scaling)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. FEATURE CORRELATION ANALYSIS\n",
    "    # ========================================\n",
    "    \n",
    "    print(f\"\\n📈 Analyzing feature relationships for GMM...\")\n",
    "    \n",
    "    correlation_matrix = X_clustering_final.corr()\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i, col1 in enumerate(correlation_matrix.columns):\n",
    "        for j, col2 in enumerate(correlation_matrix.columns):\n",
    "            if i < j and abs(correlation_matrix.loc[col1, col2]) > 0.8:\n",
    "                high_corr_pairs.append((col1, col2, correlation_matrix.loc[col1, col2]))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"   ⚠️ High correlations detected (>0.8):\")\n",
    "        for col1, col2, corr in high_corr_pairs[:3]:  # Show top 3\n",
    "            print(f\"      📊 {col1} ↔ {col2}: {corr:.3f}\")\n",
    "        if len(high_corr_pairs) > 3:\n",
    "            print(f\"      ... and {len(high_corr_pairs)-3} more\")\n",
    "    else:\n",
    "        print(f\"   ✅ No problematic feature correlations detected\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. CLUSTERING READINESS SUMMARY\n",
    "    # ========================================\n",
    "    \n",
    "    print(f\"\\n🎯 Data ready for content-agnostic GMM clustering:\")\n",
    "    print(f\"   📊 Shape: {X_clustering_final.shape}\")\n",
    "    print(f\"   📋 Features: {len(available_features)} behavioral indicators\")\n",
    "    print(f\"   💬 Comments: {len(X_clustering_final):,} from diverse content\")\n",
    "    print(f\"   🌀 Ready for: 2-component GMM with uncertainty handling\")\n",
    "    \n",
    "    return {\n",
    "        'X_clustering': X_clustering_final,\n",
    "        'text_data': text_data,\n",
    "        'original_data': original_data,\n",
    "        'feature_names': available_features,\n",
    "        'scaler': scaler,\n",
    "        'data_stats': {\n",
    "            'n_samples': len(X_clustering_final),\n",
    "            'n_features': len(available_features),\n",
    "            'missing_values': missing_values,\n",
    "            'infinite_values': infinite_values,\n",
    "            'high_correlations': len(high_corr_pairs)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Apply clustering data preparation\n",
    "if 'processed_data' in locals():\n",
    "    clustering_data = prepare_clustering_data(processed_data, scale_features=True)\n",
    "    \n",
    "    print(f\"\\n📊 Content-Agnostic Clustering Data Summary:\")\n",
    "    print(f\"   🎯 Ready for GMM with uncertainty handling\")\n",
    "    print(f\"   📈 Shape: {clustering_data['X_clustering'].shape}\")\n",
    "    print(f\"   📋 Behavioral features: {len(clustering_data['feature_names'])}\")\n",
    "    print(f\"   🌍 Works across: beauty, gaming, food, education, etc.\")\n",
    "    \n",
    "    # Show sample of prepared data with behavioral interpretation\n",
    "    print(f\"\\n📝 Sample behavioral patterns (first 3 comments):\")\n",
    "    sample_data = clustering_data['X_clustering'].head(3)\n",
    "    for i, (idx, row) in enumerate(sample_data.iterrows()):\n",
    "        comment_text = clustering_data['text_data'].iloc[i][:50] + \"...\"\n",
    "        print(f\"\\n   Comment {i+1}: \\\"{comment_text}\\\"\")\n",
    "        print(f\"   Behavioral signals:\")\n",
    "        print(f\"     📝 Text complexity: char_count={row['char_count']:.2f}, repetition={row['repetition_ratio']:.2f}\")\n",
    "        print(f\"     😊 Emoji pattern: count={row['emoji_count']:.2f}, ratio={row['emoji_ratio']:.2f}\")\n",
    "        print(f\"     🤖 Bot indicators: generic={row['is_generic']:.2f}, caps={row['caps_ratio']:.2f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Cannot prepare clustering data - processed data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd3cf4",
   "metadata": {},
   "source": [
    "## 🌀 Step 7: Primary GMM Clustering with Uncertainty Handling\n",
    "\n",
    "**Purpose**: Apply Gaussian Mixture Models following our two-stage clustering strategy with probabilistic uncertainty handling.\n",
    "\n",
    "**What this section does**:\n",
    "1. **Primary GMM (K=2)**: Separate comments into spam vs quality clusters using content-agnostic features\n",
    "2. **Uncertainty handling**: Flag 40-60% confidence comments as uncertain (manual review needed)\n",
    "3. **Cluster interpretation**: Automatically identify which cluster represents spam based on characteristics\n",
    "4. **Probabilistic validation**: Use multiple clustering quality metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin)\n",
    "5. **Business insights**: Provide actionable results with confidence scores\n",
    "\n",
    "**GMM Configuration**:\n",
    "- **n_components=2**: Only spam vs quality (uncertainty handled probabilistically)\n",
    "- **covariance_type='full'**: Allow flexible cluster shapes for diverse content\n",
    "- **Uncertainty thresholds**: 40-60% confidence flagged for manual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1fe2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gaussian Mixture Model Clustering with Uncertainty Handling\n",
    "if 'clustering_data' in locals():\n",
    "    print(\"🌀 Applying GMM Clustering with Uncertainty Handling...\\n\")\n",
    "    \n",
    "    # Extract clustering data\n",
    "    X_clustering = clustering_data['X_clustering']\n",
    "    text_data = clustering_data['text_data']\n",
    "    feature_names = clustering_data['feature_names']\n",
    "    \n",
    "    # Define uncertainty thresholds\n",
    "    UNCERTAINTY_LOWER = 0.40\n",
    "    UNCERTAINTY_UPPER = 0.60\n",
    "    \n",
    "    print(f\"🎯 Uncertainty thresholds: {UNCERTAINTY_LOWER}-{UNCERTAINTY_UPPER} (will be flagged as uncertain)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. PRIMARY GMM CLUSTERING (K=2: Spam vs Quality)\n",
    "    # ========================================\n",
    "    print(f\"\\n🎯 Stage 1: Primary clustering (Spam vs Quality)...\")\n",
    "    \n",
    "    # Initialize GMM for primary clustering\n",
    "    gmm_primary = GaussianMixture(\n",
    "        n_components=2,\n",
    "        covariance_type='full',    # Allow flexible cluster shapes\n",
    "        init_params='kmeans',      # Initialize with k-means\n",
    "        max_iter=200,             # Sufficient convergence iterations\n",
    "        n_init=10,                # Multiple random initializations for stability\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit primary clustering\n",
    "    start_time = time.time()\n",
    "    primary_labels = gmm_primary.fit_predict(X_clustering)\n",
    "    primary_probabilities = gmm_primary.predict_proba(X_clustering)\n",
    "    primary_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   ✅ Primary clustering completed in {primary_time:.2f} seconds\")\n",
    "    print(f\"   📊 Initial cluster distribution:\")\n",
    "    unique, counts = np.unique(primary_labels, return_counts=True)\n",
    "    for cluster, count in zip(unique, counts):\n",
    "        print(f\"      Cluster {cluster}: {count:,} comments ({count/len(primary_labels)*100:.1f}%)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. UNCERTAINTY HANDLING & FINAL ASSIGNMENT\n",
    "    # ========================================\n",
    "    print(f\"\\n🤔 Applying uncertainty handling...\")\n",
    "    \n",
    "    # Get maximum probabilities for each comment\n",
    "    max_probabilities = np.max(primary_probabilities, axis=1)\n",
    "    predicted_clusters = np.argmax(primary_probabilities, axis=1)\n",
    "    \n",
    "    # Apply uncertainty logic\n",
    "    def assign_final_labels_with_uncertainty(probabilities, predicted_clusters, spam_cluster_id):\n",
    "        \"\"\"Assign final labels considering uncertainty thresholds\"\"\"\n",
    "        max_probs = np.max(probabilities, axis=1)\n",
    "        final_labels = []\n",
    "        \n",
    "        for i, (max_prob, cluster) in enumerate(zip(max_probs, predicted_clusters)):\n",
    "            if max_prob > UNCERTAINTY_UPPER:\n",
    "                # Confident assignment\n",
    "                final_labels.append('spam' if cluster == spam_cluster_id else 'quality')\n",
    "            else:\n",
    "                # Uncertain - needs manual review\n",
    "                final_labels.append('uncertain')\n",
    "        \n",
    "        return np.array(final_labels)\n",
    "    \n",
    "    # First, determine which cluster is spam vs quality\n",
    "    cluster_stats = {}\n",
    "    for cluster_id in [0, 1]:\n",
    "        mask = primary_labels == cluster_id\n",
    "        cluster_data = X_clustering[mask]\n",
    "        \n",
    "        # Calculate cluster statistics to identify spam cluster\n",
    "        stats = {\n",
    "            'size': mask.sum(),\n",
    "            'avg_char_count': cluster_data['char_count'].mean(),\n",
    "            'generic_percentage': cluster_data['is_generic'].mean() * 100,\n",
    "            'avg_caps_ratio': cluster_data['caps_ratio'].mean(),\n",
    "            'avg_emoji_ratio': cluster_data['emoji_ratio'].mean(),\n",
    "        }\n",
    "        cluster_stats[cluster_id] = stats\n",
    "    \n",
    "    # Identify spam cluster (higher generic %, lower char count, higher caps ratio)\n",
    "    spam_score_0 = (cluster_stats[0]['generic_percentage'] + \n",
    "                    cluster_stats[0]['avg_caps_ratio'] * 100 - \n",
    "                    cluster_stats[0]['avg_char_count'] / 10)\n",
    "    spam_score_1 = (cluster_stats[1]['generic_percentage'] + \n",
    "                    cluster_stats[1]['avg_caps_ratio'] * 100 - \n",
    "                    cluster_stats[1]['avg_char_count'] / 10)\n",
    "    \n",
    "    spam_cluster_id = 0 if spam_score_0 > spam_score_1 else 1\n",
    "    quality_cluster_id = 1 - spam_cluster_id\n",
    "    \n",
    "    print(f\"   🔍 Identified Cluster {spam_cluster_id} as SPAM-LIKE\")\n",
    "    print(f\"   🔍 Identified Cluster {quality_cluster_id} as QUALITY-LIKE\")\n",
    "    \n",
    "    # Apply final labeling with uncertainty\n",
    "    final_labels = assign_final_labels_with_uncertainty(\n",
    "        primary_probabilities, predicted_clusters, spam_cluster_id\n",
    "    )\n",
    "    \n",
    "    # Calculate distribution\n",
    "    unique_final, counts_final = np.unique(final_labels, return_counts=True)\n",
    "    total_comments = len(final_labels)\n",
    "    \n",
    "    print(f\"\\n📊 Final distribution with uncertainty handling:\")\n",
    "    for label, count in zip(unique_final, counts_final):\n",
    "        percentage = count / total_comments * 100\n",
    "        print(f\"      {label.title()}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. UNCERTAINTY ANALYSIS\n",
    "    # ========================================\n",
    "    print(f\"\\n🔍 Analyzing uncertain comments...\")\n",
    "    \n",
    "    uncertain_mask = final_labels == 'uncertain'\n",
    "    if uncertain_mask.sum() > 0:\n",
    "        uncertain_probs = max_probabilities[uncertain_mask]\n",
    "        uncertain_texts = text_data[uncertain_mask]\n",
    "        \n",
    "        print(f\"   📊 Uncertainty statistics:\")\n",
    "        print(f\"      🤔 Uncertain rate: {uncertain_mask.mean()*100:.1f}%\")\n",
    "        print(f\"      📈 Avg confidence: {uncertain_probs.mean():.3f}\")\n",
    "        print(f\"      📉 Min confidence: {uncertain_probs.min():.3f}\")\n",
    "        print(f\"      📊 Max confidence: {uncertain_probs.max():.3f}\")\n",
    "        \n",
    "        # Show examples of uncertain comments\n",
    "        print(f\"\\n   📝 Sample uncertain comments:\")\n",
    "        uncertain_indices = np.where(uncertain_mask)[0][:3]\n",
    "        for i, idx in enumerate(uncertain_indices):\n",
    "            text = text_data.iloc[idx]\n",
    "            prob = max_probabilities[idx]\n",
    "            cluster = predicted_clusters[idx]\n",
    "            cluster_name = 'spam' if cluster == spam_cluster_id else 'quality'\n",
    "            print(f\"      {i+1}. \\\"{text[:60]}...\\\" \")\n",
    "            print(f\"         → {prob:.3f} confidence toward {cluster_name}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. CLUSTERING QUALITY METRICS\n",
    "    # ========================================\n",
    "    print(f\"\\n📈 Evaluating clustering quality...\")\n",
    "    \n",
    "    # Calculate clustering metrics (using original binary clustering)\n",
    "    silhouette_avg = silhouette_score(X_clustering, primary_labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_clustering, primary_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_clustering, primary_labels)\n",
    "    \n",
    "    print(f\"   📊 Silhouette Score: {silhouette_avg:.3f} (higher is better, >0.5 is good)\")\n",
    "    print(f\"   📊 Calinski-Harabasz Index: {calinski_harabasz:.1f} (higher is better)\")\n",
    "    print(f\"   📊 Davies-Bouldin Index: {davies_bouldin:.3f} (lower is better)\")\n",
    "    \n",
    "    # Model selection metrics\n",
    "    aic = gmm_primary.aic(X_clustering)\n",
    "    bic = gmm_primary.bic(X_clustering)\n",
    "    log_likelihood = gmm_primary.score(X_clustering)\n",
    "    \n",
    "    print(f\"   📊 AIC: {aic:.1f} (lower is better)\")\n",
    "    print(f\"   📊 BIC: {bic:.1f} (lower is better)\")\n",
    "    print(f\"   📊 Log-likelihood: {log_likelihood:.1f} (higher is better)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. CONFIDENT COMMENTS ANALYSIS\n",
    "    # ========================================\n",
    "    print(f\"\\n✅ Analyzing confident predictions...\")\n",
    "    \n",
    "    # Analyze confident spam and quality comments\n",
    "    confident_spam_mask = final_labels == 'spam'\n",
    "    confident_quality_mask = final_labels == 'quality'\n",
    "    \n",
    "    if confident_spam_mask.sum() > 0:\n",
    "        print(f\"\\n   🚫 Confident Spam Examples:\")\n",
    "        spam_indices = np.where(confident_spam_mask)[0][:3]\n",
    "        for i, idx in enumerate(spam_indices):\n",
    "            text = text_data.iloc[idx]\n",
    "            prob = max_probabilities[idx]\n",
    "            print(f\"      {i+1}. \\\"{text}\\\" (confidence: {prob:.3f})\")\n",
    "    \n",
    "    if confident_quality_mask.sum() > 0:\n",
    "        print(f\"\\n   ✅ Confident Quality Examples:\")\n",
    "        quality_indices = np.where(confident_quality_mask)[0][:3]\n",
    "        for i, idx in enumerate(quality_indices):\n",
    "            text = text_data.iloc[idx][:80] + \"...\"\n",
    "            prob = max_probabilities[idx]\n",
    "            print(f\"      {i+1}. \\\"{text}\\\" (confidence: {prob:.3f})\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 6. STORE RESULTS\n",
    "    # ========================================\n",
    "    \n",
    "    # Store results for later use\n",
    "    clustering_results = {\n",
    "        'gmm_primary': gmm_primary,\n",
    "        'primary_labels': primary_labels,\n",
    "        'primary_probabilities': primary_probabilities,\n",
    "        'final_labels': final_labels,\n",
    "        'max_probabilities': max_probabilities,\n",
    "        'spam_cluster_id': spam_cluster_id,\n",
    "        'quality_cluster_id': quality_cluster_id,\n",
    "        'uncertainty_thresholds': {\n",
    "            'lower': UNCERTAINTY_LOWER,\n",
    "            'upper': UNCERTAINTY_UPPER\n",
    "        },\n",
    "        'cluster_stats': cluster_stats,\n",
    "        'metrics': {\n",
    "            'silhouette': silhouette_avg,\n",
    "            'calinski_harabasz': calinski_harabasz,\n",
    "            'davies_bouldin': davies_bouldin,\n",
    "            'aic': aic,\n",
    "            'bic': bic,\n",
    "            'uncertainty_rate': uncertain_mask.mean()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✅ GMM clustering with uncertainty handling completed!\")\n",
    "    print(f\"🎯 Results summary:\")\n",
    "    print(f\"   🚫 Confident spam: {confident_spam_mask.sum():,} ({confident_spam_mask.mean()*100:.1f}%)\")\n",
    "    print(f\"   ✅ Confident quality: {confident_quality_mask.sum():,} ({confident_quality_mask.mean()*100:.1f}%)\")\n",
    "    print(f\"   🤔 Uncertain (needs review): {uncertain_mask.sum():,} ({uncertain_mask.mean()*100:.1f}%)\")\n",
    "    print(f\"   📊 Clustering quality (Silhouette): {silhouette_avg:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot perform clustering - clustering data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8448ef1",
   "metadata": {},
   "source": [
    "## 📊 Step 8: Visualize Clustering Results with Uncertainty Analysis\n",
    "\n",
    "**Purpose**: Create comprehensive visualizations to validate our content-agnostic clustering results and uncertainty handling.\n",
    "\n",
    "**What this section does**:\n",
    "1. **PCA visualization**: Project high-dimensional clusters into 2D space showing spam/quality/uncertain\n",
    "2. **Uncertainty analysis**: Visualize confidence distributions and uncertainty zones\n",
    "3. **Feature importance**: Show which features best separate clusters across content domains\n",
    "4. **Business dashboard**: Provide actionable insights with confidence metrics\n",
    "5. **Cross-domain validation**: Ensure clustering works across beauty, gaming, food, education content\n",
    "\n",
    "**Key Visualizations**:\n",
    "- **Three-color PCA**: Red=Spam, Blue=Quality, Orange=Uncertain\n",
    "- **Confidence histograms**: Distribution by category with uncertainty zone marked\n",
    "- **Uncertainty rate analysis**: Manual review queue sizing\n",
    "- **Feature importance**: Content-agnostic discriminative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Clustering Results with Uncertainty\n",
    "if 'clustering_results' in locals():\n",
    "    print(\"📊 Creating clustering visualizations with uncertainty handling...\\n\")\n",
    "    \n",
    "    # Extract data\n",
    "    X_clustering = clustering_data['X_clustering']\n",
    "    primary_labels = clustering_results['primary_labels']\n",
    "    final_labels = clustering_results['final_labels']\n",
    "    max_probabilities = clustering_results['max_probabilities']\n",
    "    spam_cluster_id = clustering_results['spam_cluster_id']\n",
    "    uncertainty_thresholds = clustering_results['uncertainty_thresholds']\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. PCA VISUALIZATION WITH UNCERTAINTY\n",
    "    # ========================================\n",
    "    print(\"🎯 Creating PCA visualization with uncertainty...\")\n",
    "    \n",
    "    # Apply PCA for 2D visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_clustering)\n",
    "    \n",
    "    # Create enhanced PCA plots\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Plot 1: Three-way classification (spam/quality/uncertain)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    \n",
    "    # Define colors for three categories\n",
    "    color_map = {\n",
    "        'spam': 'red',\n",
    "        'quality': 'blue', \n",
    "        'uncertain': 'orange'\n",
    "    }\n",
    "    colors = [color_map[label] for label in final_labels]\n",
    "    \n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6, s=20)\n",
    "    plt.xlabel(f'PCA Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PCA Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.title('Comments with Uncertainty Handling\\n(Red=Spam, Blue=Quality, Orange=Uncertain)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    import matplotlib.patches as mpatches\n",
    "    red_patch = mpatches.Patch(color='red', label='Confident Spam')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Confident Quality')\n",
    "    orange_patch = mpatches.Patch(color='orange', label='Uncertain')\n",
    "    plt.legend(handles=[red_patch, blue_patch, orange_patch], loc='upper right')\n",
    "    \n",
    "    # Plot 2: Confidence scores with uncertainty zone\n",
    "    plt.subplot(1, 3, 2)\n",
    "    scatter2 = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=max_probabilities, \n",
    "                          cmap='viridis', alpha=0.7, s=20)\n",
    "    plt.xlabel(f'PCA Component 1')\n",
    "    plt.ylabel(f'PCA Component 2')\n",
    "    plt.title('Clustering Confidence Scores\\n(Dark=Low Confidence/Uncertain)')\n",
    "    cbar = plt.colorbar(scatter2, label='Max Probability')\n",
    "    \n",
    "    # Add uncertainty zone markers\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Distribution of confidence scores\n",
    "    plt.subplot(1, 3, 3)\n",
    "    \n",
    "    # Create histogram of confidence scores\n",
    "    plt.hist(max_probabilities, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Add uncertainty zone\n",
    "    plt.axvspan(uncertainty_thresholds['lower'], uncertainty_thresholds['upper'], \n",
    "                alpha=0.3, color='orange', label=f'Uncertainty Zone\\n({uncertainty_thresholds[\"lower\"]}-{uncertainty_thresholds[\"upper\"]})')\n",
    "    \n",
    "    plt.xlabel('Maximum Probability')\n",
    "    plt.ylabel('Number of Comments')\n",
    "    plt.title('Distribution of Clustering Confidence')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   ✅ PCA visualization completed\")\n",
    "    print(f\"   📊 PCA explains {pca.explained_variance_ratio_.sum():.1%} of total variance\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. UNCERTAINTY ANALYSIS PLOTS\n",
    "    # ========================================\n",
    "    print(f\"\\n🤔 Creating uncertainty analysis plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Final label distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    final_counts = pd.Series(final_labels).value_counts()\n",
    "    colors_pie = [color_map[label] for label in final_counts.index]\n",
    "    wedges, texts, autotexts = ax1.pie(final_counts.values, labels=final_counts.index, \n",
    "                                      autopct='%1.1f%%', colors=colors_pie)\n",
    "    ax1.set_title('Final Classification Distribution')\n",
    "    \n",
    "    # Plot 2: Confidence score distribution by category\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    for label, color in color_map.items():\n",
    "        mask = final_labels == label\n",
    "        if mask.sum() > 0:\n",
    "            probs = max_probabilities[mask]\n",
    "            ax2.hist(probs, alpha=0.6, label=f'{label.title()} (n={mask.sum():,})', \n",
    "                    color=color, bins=15)\n",
    "    \n",
    "    ax2.axvspan(uncertainty_thresholds['lower'], uncertainty_thresholds['upper'], \n",
    "                alpha=0.2, color='gray', label='Uncertainty Zone')\n",
    "    ax2.set_xlabel('Maximum Probability')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Confidence Distribution by Category')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Feature importance for separation\n",
    "    ax3 = axes[1, 0]\n",
    "    feature_importance = np.abs(pca.components_).mean(axis=0)\n",
    "    top_features_idx = np.argsort(feature_importance)[-8:]  # Top 8 features\n",
    "    top_features = [clustering_data['feature_names'][i] for i in top_features_idx]\n",
    "    top_importance = feature_importance[top_features_idx]\n",
    "    \n",
    "    bars = ax3.barh(range(len(top_features)), top_importance)\n",
    "    ax3.set_yticks(range(len(top_features)))\n",
    "    ax3.set_yticklabels(top_features)\n",
    "    ax3.set_xlabel('PCA Loading (Importance)')\n",
    "    ax3.set_title('Top Features for Clustering')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Uncertainty rate by confidence bins\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Create confidence bins\n",
    "    bins = np.linspace(0.5, 1.0, 11)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    uncertainty_rates = []\n",
    "    \n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (max_probabilities >= bins[i]) & (max_probabilities < bins[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            uncertainty_rate = (final_labels[mask] == 'uncertain').mean()\n",
    "            uncertainty_rates.append(uncertainty_rate)\n",
    "        else:\n",
    "            uncertainty_rates.append(0)\n",
    "    \n",
    "    bars = ax4.bar(bin_centers, uncertainty_rates, width=0.04, alpha=0.7, color='orange')\n",
    "    ax4.set_xlabel('Confidence Score')\n",
    "    ax4.set_ylabel('Uncertainty Rate')\n",
    "    ax4.set_title('Uncertainty Rate by Confidence Level')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add uncertainty threshold lines\n",
    "    ax4.axvline(uncertainty_thresholds['upper'], color='red', linestyle='--', \n",
    "                label=f'Threshold: {uncertainty_thresholds[\"upper\"]}')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. BUSINESS INSIGHTS WITH UNCERTAINTY\n",
    "    # ========================================\n",
    "    print(f\"\\n💼 Business Insights with Uncertainty Handling:\")\n",
    "    \n",
    "    total_comments = len(final_labels)\n",
    "    spam_count = (final_labels == 'spam').sum()\n",
    "    quality_count = (final_labels == 'quality').sum()\n",
    "    uncertain_count = (final_labels == 'uncertain').sum()\n",
    "    \n",
    "    print(f\"   📊 Comment Classification Results:\")\n",
    "    print(f\"      🚫 Confident Spam: {spam_count:,} ({spam_count/total_comments*100:.1f}%)\")\n",
    "    print(f\"      ✅ Confident Quality: {quality_count:,} ({quality_count/total_comments*100:.1f}%)\")\n",
    "    print(f\"      🤔 Uncertain (Manual Review): {uncertain_count:,} ({uncertain_count/total_comments*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n   🎯 Uncertainty Analysis:\")\n",
    "    if uncertain_count > 0:\n",
    "        uncertain_probs = max_probabilities[final_labels == 'uncertain']\n",
    "        print(f\"      📈 Average uncertainty confidence: {uncertain_probs.mean():.3f}\")\n",
    "        print(f\"      📊 Uncertainty range: {uncertain_probs.min():.3f} - {uncertain_probs.max():.3f}\")\n",
    "        print(f\"      💡 These {uncertain_count:,} comments need manual review\")\n",
    "    \n",
    "    print(f\"\\n   🔍 Model Confidence:\")\n",
    "    confident_mask = final_labels != 'uncertain'\n",
    "    if confident_mask.sum() > 0:\n",
    "        confident_probs = max_probabilities[confident_mask]\n",
    "        print(f\"      ✅ Average confidence for decided comments: {confident_probs.mean():.3f}\")\n",
    "        print(f\"      🎯 {confident_mask.mean()*100:.1f}% of comments classified with confidence\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    metrics = clustering_results['metrics']\n",
    "    print(f\"\\n   📊 Clustering Quality Metrics:\")\n",
    "    print(f\"      🎯 Silhouette Score: {metrics['silhouette']:.3f}\")\n",
    "    print(f\"      📈 Uncertainty Rate: {metrics['uncertainty_rate']*100:.1f}%\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\n   💡 Interpretation:\")\n",
    "    if metrics['uncertainty_rate'] < 0.1:\n",
    "        print(f\"      ✅ Low uncertainty rate (<10%) indicates clear cluster separation\")\n",
    "    elif metrics['uncertainty_rate'] < 0.2:\n",
    "        print(f\"      ⚠️ Moderate uncertainty rate (10-20%) - some ambiguous cases\")\n",
    "    else:\n",
    "        print(f\"      🚨 High uncertainty rate (>20%) - may need feature engineering review\")\n",
    "    \n",
    "    print(f\"\\n✅ Uncertainty-aware clustering analysis completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot create visualizations - clustering results not available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
