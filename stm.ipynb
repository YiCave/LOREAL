{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c42fb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded comments: 3,326,124 rows\n",
      "Loaded videos: 92,759 rows\n",
      "\n",
      "Unique videoIds in comments: 36,375\n",
      "Unique videoIds in videos: 92,759\n",
      "\n",
      "MERGE STATISTICS:\n",
      "Videos without any comments: 56,472\n",
      "Comments referencing non-existent videos: 88\n",
      "\n",
      "Example videos without comments: [np.int64(1), np.int64(3), np.int64(4), np.int64(5), np.int64(7)]\n",
      "Example videoIds in comments but not in videos: [np.int64(0), np.int64(89600), np.int64(59912), np.int64(53272), np.int64(61977)]\n",
      "Loaded videos: 92,759 rows\n",
      "\n",
      "Unique videoIds in comments: 36,375\n",
      "Unique videoIds in videos: 92,759\n",
      "\n",
      "MERGE STATISTICS:\n",
      "Videos without any comments: 56,472\n",
      "Comments referencing non-existent videos: 88\n",
      "\n",
      "Example videos without comments: [np.int64(1), np.int64(3), np.int64(4), np.int64(5), np.int64(7)]\n",
      "Example videoIds in comments but not in videos: [np.int64(0), np.int64(89600), np.int64(59912), np.int64(53272), np.int64(61977)]\n",
      "\n",
      "MERGE RESULTS:\n",
      "Comments before merge: 3,326,124\n",
      "Comments after merge: 3,325,035\n",
      "Comments lost in merge: 1,089\n",
      "Videos included in merge: 36,287\n",
      "\n",
      "Comment IDs preserved: All 3,325,035 comment IDs remain intact\n",
      "Each row still represents one comment, now enriched with video metadata\n",
      "\n",
      "MERGED DATASET COLUMNS:\n",
      "Comment columns: ['kind_comment', 'commentId', 'channelId_comment', 'textOriginal', 'likeCount_comment', 'publishedAt_comment']\n",
      "Video columns: ['kind_video', 'publishedAt_video', 'channelId_video', 'title', 'description', 'tags', 'likeCount_video']\n",
      "Shared columns: ['commentId', 'authorId', 'textOriginal', 'parentCommentId', 'updatedAt', 'char_count', 'word_count', 'caps_ratio', 'repetition_ratio', 'emoji_ratio', 'emoji_diversity', 'likes_per_char', 'is_reply', 'url_count', 'is_generic', 'spam_classification', 'classification_confidence', 'title', 'description', 'tags', 'defaultLanguage', 'defaultAudioLanguage', 'contentDuration', 'viewCount', 'favouriteCount', 'commentCount', 'topicCategories']\n",
      "\n",
      "SAMPLE OF MERGED DATA:\n",
      "====================================================================================================\n",
      " commentId  videoId                                        textOriginal                                                                                             title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           description  likeCount_comment  viewCount\n",
      "   1781382    74288 PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would rock it           I tried hair inspired by the PAN flag 🩷💛🩵 #pansexual #transandproud #hairtransformation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN                  0  9856583.0\n",
      "    289571    79618    Apply mashed potato juice and mixed it with curd                                                 5 Foundation Mistakes that Every Girl Should Know                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                5 Foundation Mistakes that Every Girl Should Know\\n\\nWhatsApp 9891002929/9953702929\\n\\n#rohitsachdeva #beautyhacks #beautytips #beautyinfluencer #trendingbeautytips #viralbeautytricks #viralbeautytips #skinwhitening #glassskin #glassskinchallenge #glassskincare #darkspots #homeremedies #glowingskin #glowmask #hydratedskin #healthyglowingskin #viralshorts #musttrythisathome #trendingshorts #trendingshort                  0  1148157.0\n",
      "    569077    51826                          69 missed calls from mars👽                                                                How To Make Small Eyes Look Bigger                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    How To Make Small Eyes Look Bigger                  0 14590307.0\n",
      "   2957962    58298                                                Baaa 20sec beauty test: BLUSH PLACEMENT for YOUR FACE! 💋 #blush #kbeauty #koreanmakeup #douyin #shorts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN                  0   153902.0\n",
      "    673093     1265     you look like raven from phenomena raven no cap                                                              BLACK GIRL TRIES KYLIE JENNER MAKEUP Today on Black Girl Tries we are trying Kylie Jenner's Everyday Makeup Look\\n\\n\"Chanel\" tweed jacket: http://shrsl.com/22sxx (use code DARCEI10)\\n\\nPOTATO GANG MERCH ON SALE NOW: https://shop.missdarcei.com/\\n\\nKylie Skin Hydrating Lip Balm Mask\\nhttps://shop-links.co/1738420325012109289 \\n\\nKylie Skin Face Moisturizer\\nhttps://shop-links.co/1738420349119487971 \\n\\nNARS All Day Luminous Weightless Foundation\\nhttps://shop-links.co/1738420358114206889 \\n\\nBeautyblender Makeup Sponge\\nhttps://shop-links.co/1738420491181783888 \\n\\nKylie Cosmetics Translucent Loose Setting Powder\\nhttps://shop-links.co/1738420589494362861 \\n\\nAnastasia Beverly Hills Brow Wiz\\nhttps://shop-links.co/1738420680327247449 \\n\\nKylie Cosmetics Kybrow Highlighter\\nhttps://shop-links.co/1738420792050709740 \\n\\nKylie Cosmetics The Bronze Palette\\nhttps://shop-links.co/1738420860897007232 \\n\\nKylie Cosmetics Shimmer Eye Glaze\\nhttps://shop-links.co/1738420921675110251 \\n\\nKylie Cosmetics Bronzer\\nhttps://shop-links.co/1738421027158256948 \\n\\nKylie Cosmetics Blush\\nhttps://shop-links.co/1738421143440164461 \\n\\nKylie Cosmetics Matte Lip Kit\\nhttps://shop-links.co/1738421231964449299 \\n\\nKylie Cosmetics Setting Spray\\nhttps://shop-links.co/1738421283312134931 \\n\\nWhy hello there! I'm Darcei and welcome to MissDarcei where we talk about everything Korean Beauty related. Korean makeup tutorials, Korean makeup reviews, Korean skincare. Subscribe to watch this black girl try it all!\\n\\nFOLLOW ME :)\\n \\nFACEBOOK\\nhttp://www.facebook.com/missdarcei\\n\\nINSTAGRAM\\nhttp://www.instagram.com/missdarcei\\n\\nTWITTER\\nhttp://www.twitter.com/missdarcei\\n\\nSNAPCHAT\\n@MissDarcei\\n\\n\\n\\n\\nMissDarcei Business Enquiries: Perlman.mgmt@gmail.com                  0 12347504.0\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Grouping two datasets by videoid first / also known as DOCUMENT POOLING-> objective:to fix small text issue for LDA to make sure it can work\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix numpy compatibility issue\n",
    "import numpy\n",
    "if not hasattr(numpy, '_no_nep50_warning'):\n",
    "    numpy._no_nep50_warning = lambda: None\n",
    "\n",
    "# Load the latest processed comments (from your spam detection pipeline)\n",
    "comments_df = pd.read_csv('complete_comments_top20_features.csv')\n",
    "print(f\"Loaded comments: {len(comments_df):,} rows\")\n",
    "\n",
    "# Load original videos dataset\n",
    "videos_df = pd.read_csv('../dataset/videos.csv')\n",
    "print(f\"Loaded videos: {len(videos_df):,} rows\")\n",
    "\n",
    "# Check unique videoIds in each dataset\n",
    "unique_videos_in_comments = comments_df['videoId'].nunique()\n",
    "unique_videos_in_videos = videos_df['videoId'].nunique()\n",
    "\n",
    "print(f\"\\nUnique videoIds in comments: {unique_videos_in_comments:,}\")\n",
    "print(f\"Unique videoIds in videos: {unique_videos_in_videos:,}\")\n",
    "\n",
    "# Find videos without comments and comments without videos\n",
    "videos_with_comments = set(comments_df['videoId'].unique())\n",
    "videos_in_dataset = set(videos_df['videoId'].unique())\n",
    "\n",
    "videos_without_comments = videos_in_dataset - videos_with_comments\n",
    "comments_without_videos = videos_with_comments - videos_in_dataset\n",
    "\n",
    "print(f\"\\nMERGE STATISTICS:\")\n",
    "print(f\"Videos without any comments: {len(videos_without_comments):,}\")\n",
    "print(f\"Comments referencing non-existent videos: {len(comments_without_videos):,}\")\n",
    "\n",
    "# Show some examples if they exist\n",
    "if videos_without_comments:\n",
    "    print(f\"\\nExample videos without comments: {list(videos_without_comments)[:5]}\")\n",
    "if comments_without_videos:\n",
    "    print(f\"Example videoIds in comments but not in videos: {list(comments_without_videos)[:5]}\")\n",
    "\n",
    "# Perform the merge - keeping all comments that have matching videos\n",
    "merged_df = comments_df.merge(videos_df, on='videoId', how='inner', suffixes=('_comment', '_video'))\n",
    "\n",
    "print(f\"\\nMERGE RESULTS:\")\n",
    "print(f\"Comments before merge: {len(comments_df):,}\")\n",
    "print(f\"Comments after merge: {len(merged_df):,}\")\n",
    "print(f\"Comments lost in merge: {len(comments_df) - len(merged_df):,}\")\n",
    "print(f\"Videos included in merge: {merged_df['videoId'].nunique():,}\")\n",
    "\n",
    "# Show what happens to comment IDs - they are preserved!\n",
    "print(f\"\\nComment IDs preserved: All {len(merged_df):,} comment IDs remain intact\")\n",
    "print(f\"Each row still represents one comment, now enriched with video metadata\")\n",
    "\n",
    "print(f\"\\nMERGED DATASET COLUMNS:\")\n",
    "print(\"Comment columns:\", [col for col in merged_df.columns if col.endswith('_comment') or col in ['commentId', 'textOriginal', 'likeCount_comment']])\n",
    "print(\"Video columns:\", [col for col in merged_df.columns if col.endswith('_video') or col in ['title', 'description', 'tags']])\n",
    "print(\"Shared columns:\", [col for col in merged_df.columns if not col.endswith('_comment') and not col.endswith('_video') and col != 'videoId'])\n",
    "\n",
    "# Display merged dataframe in table format\n",
    "print(\"\\nSAMPLE OF MERGED DATA:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Select key columns for easier viewing\n",
    "key_columns = ['commentId', 'videoId', 'textOriginal', 'title', 'description', 'likeCount_comment', 'viewCount']\n",
    "display_df = merged_df[key_columns].head()\n",
    "\n",
    "# Set pandas display options for better table formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39971220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Starting document pooling with 3,325,035 comments\n",
      "Available columns in merged_df:\n",
      "['kind_comment', 'commentId', 'channelId_comment', 'videoId', 'authorId', 'textOriginal', 'parentCommentId', 'likeCount_comment', 'publishedAt_comment', 'updatedAt', 'char_count', 'word_count', 'caps_ratio', 'repetition_ratio', 'emoji_ratio', 'emoji_diversity', 'likes_per_char', 'is_reply', 'url_count', 'is_generic', 'spam_classification', 'classification_confidence', 'kind_video', 'publishedAt_video', 'channelId_video', 'title', 'description', 'tags', 'defaultLanguage', 'defaultAudioLanguage', 'contentDuration', 'viewCount', 'likeCount_video', 'favouriteCount', 'commentCount', 'topicCategories']\n",
      "\n",
      "Found spam classification column: 'spam_classification'\n",
      "Unique values in spam_classification: {'quality': np.int64(3087679), 'spam': np.int64(235289), 'uncertain': np.int64(2067)}\n",
      "Using 3,087,679 quality comments from 34,949 videos\n",
      "Using 3,087,679 quality comments from 34,949 videos\n",
      "\n",
      "Document Pooling Results:\n",
      "Created 34949 pooled documents\n",
      "Average document length: 4265 characters\n",
      "Total characters: 149,069,831\n",
      "\n",
      "Sample pooled document:\n",
      "================================================================================\n",
      "TITLE: Let’s give happiness #toupee #barber #hairlosstreatment #barbershop DESCRIPTION: nan COMMENTS: Where do you go to get something like this\n",
      "================================================================================\n",
      "\n",
      "Document Pooling Results:\n",
      "Created 34949 pooled documents\n",
      "Average document length: 4265 characters\n",
      "Total characters: 149,069,831\n",
      "\n",
      "Sample pooled document:\n",
      "================================================================================\n",
      "TITLE: Let’s give happiness #toupee #barber #hairlosstreatment #barbershop DESCRIPTION: nan COMMENTS: Where do you go to get something like this\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Document Pooling - Create mega-documents per video for LDA topic modeling\n",
    "\n",
    "# Safety check - ensure required variables exist\n",
    "if 'merged_df' not in locals():\n",
    "    print(\"❌ Error: merged_df not found. Please run cell 1 first!\")\n",
    "    raise NameError(\"merged_df not defined. Run the previous cell first.\")\n",
    "\n",
    "print(f\"✅ Starting document pooling with {len(merged_df):,} comments\")\n",
    "\n",
    "# Check available columns first\n",
    "print(\"Available columns in merged_df:\")\n",
    "print(merged_df.columns.tolist())\n",
    "print()\n",
    "\n",
    "# Filter for quality comments only (remove spam for clean topic modeling)\n",
    "spam_col = None\n",
    "if 'spam_classification' in merged_df.columns:\n",
    "    spam_col = 'spam_classification'\n",
    "elif 'final_labels' in merged_df.columns:\n",
    "    spam_col = 'final_labels'\n",
    "\n",
    "if spam_col:\n",
    "    # Check what values are in the spam classification column\n",
    "    print(f\"Found spam classification column: '{spam_col}'\")\n",
    "    unique_values = merged_df[spam_col].value_counts()\n",
    "    print(f\"Unique values in {spam_col}: {dict(unique_values)}\")\n",
    "    \n",
    "    # Filter for quality comments only\n",
    "    quality_comments = merged_df[merged_df[spam_col] == 'quality'].copy()\n",
    "    print(f\"Using {len(quality_comments):,} quality comments from {quality_comments['videoId'].nunique():,} videos\")\n",
    "else:\n",
    "    # No spam classification available - use all comments\n",
    "    print(\"⚠️  No spam classification found. Using ALL comments for topic modeling.\")\n",
    "    print(\"   Run the spam detection pipeline first for better results.\")\n",
    "    quality_comments = merged_df.copy()\n",
    "    print(f\"Using {len(quality_comments):,} total comments from {quality_comments['videoId'].nunique():,} videos\")\n",
    "\n",
    "# Group by videoId and create pooled documents\n",
    "pooled_documents = []\n",
    "video_metadata = []\n",
    "\n",
    "for video_id, group in quality_comments.groupby('videoId'):\n",
    "    # Get video metadata (first row since all rows have same video info)\n",
    "    video_info = group.iloc[0]\n",
    "    \n",
    "    # Collect all comment texts for this video\n",
    "    comment_texts = group['textOriginal'].fillna('').tolist()\n",
    "    \n",
    "    # Create the pooled document: video title + description + all comments\n",
    "    video_title = str(video_info.get('title', '')).strip()\n",
    "    video_description = str(video_info.get('description', '')).strip()\n",
    "    \n",
    "    # Combine all text for this video\n",
    "    pooled_text_parts = []\n",
    "    \n",
    "    if video_title:\n",
    "        pooled_text_parts.append(f\"TITLE: {video_title}\")\n",
    "    \n",
    "    if video_description:\n",
    "        pooled_text_parts.append(f\"DESCRIPTION: {video_description}\")\n",
    "    \n",
    "    # Add all quality comments\n",
    "    if comment_texts:\n",
    "        comment_text = \" \".join([str(text).strip() for text in comment_texts if str(text).strip()])\n",
    "        if comment_text:\n",
    "            pooled_text_parts.append(f\"COMMENTS: {comment_text}\")\n",
    "    \n",
    "    # Create final pooled document\n",
    "    pooled_document = \" \".join(pooled_text_parts)\n",
    "    \n",
    "    if pooled_document.strip():  # Only add if document has content\n",
    "        pooled_documents.append(pooled_document)\n",
    "        \n",
    "        # Store metadata for this video-document\n",
    "        video_metadata.append({\n",
    "            'videoId': video_id,\n",
    "            'title': video_title,\n",
    "            'description': video_description,\n",
    "            'num_comments': len(comment_texts),\n",
    "            'viewCount': video_info.get('viewCount', 0),\n",
    "            'likeCount_video': video_info.get('likeCount_video', 0),\n",
    "            'document_length': len(pooled_document),\n",
    "            'publishedAt_video': video_info.get('publishedAt_video', '')\n",
    "        })\n",
    "\n",
    "print(f\"\\nDocument Pooling Results:\")\n",
    "print(f\"Created {len(pooled_documents)} pooled documents\")\n",
    "print(f\"Average document length: {np.mean([len(doc) for doc in pooled_documents]):.0f} characters\")\n",
    "print(f\"Total characters: {sum(len(doc) for doc in pooled_documents):,}\")\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "pooled_df = pd.DataFrame(video_metadata)\n",
    "pooled_df['pooled_text'] = pooled_documents\n",
    "\n",
    "print(f\"\\nSample pooled document:\")\n",
    "print(\"=\"*80)\n",
    "sample_doc = pooled_documents[0][:500] + \"...\" if len(pooled_documents[0]) > 500 else pooled_documents[0]\n",
    "print(sample_doc)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44b16e66",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_torch_array' from 'scipy._lib.array_api_compat.common._helpers' (c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\_lib\\array_api_compat\\common\\_helpers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Topic Modeling - Import libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora, models\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoherencemodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoherenceModel\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\parsing\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     preprocess_documents,\n\u001b[0;32m      6\u001b[0m     preprocess_string,\n\u001b[0;32m      7\u001b[0m     read_file,\n\u001b[0;32m      8\u001b[0m     read_files,\n\u001b[0;32m      9\u001b[0m     remove_stopwords,\n\u001b[0;32m     10\u001b[0m     split_alphanum,\n\u001b[0;32m     11\u001b[0m     stem_text,\n\u001b[0;32m     12\u001b[0m     strip_multiple_whitespaces,\n\u001b[0;32m     13\u001b[0m     strip_non_alphanum,\n\u001b[0;32m     14\u001b[0m     strip_numeric,\n\u001b[0;32m     15\u001b[0m     strip_punctuation,\n\u001b[0;32m     16\u001b[0m     strip_short,\n\u001b[0;32m     17\u001b[0m     strip_tags,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\parsing\\preprocessing.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m     30\u001b[0m STOPWORDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mover\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manyway\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthrough\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfifty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msomewhere\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monce\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     59\u001b[0m ])\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\utils.py:36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msmart_open\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\__init__.py:300\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Base class for sparse matrices\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0;32m      6\u001b[0m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[0;32m      7\u001b[0m                        matrix, validateaxis, getdtype)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[0;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misspmatrix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missparse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparray\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseWarning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseEfficiencyWarning\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_sputils.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prod\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msp\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[0;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupcast\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misscalarlike\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misintlike\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misshape\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missequence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misdense\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mismatrix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_sum_dtype\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbroadcast_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m supported_dtypes \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mbool_, np\u001b[38;5;241m.\u001b[39mbyte, np\u001b[38;5;241m.\u001b[39mubyte, np\u001b[38;5;241m.\u001b[39mshort, np\u001b[38;5;241m.\u001b[39mushort, np\u001b[38;5;241m.\u001b[39mintc,\n\u001b[0;32m     18\u001b[0m                     np\u001b[38;5;241m.\u001b[39muintc, np_long, np_ulong, np\u001b[38;5;241m.\u001b[39mlonglong, np\u001b[38;5;241m.\u001b[39mulonglong,\n\u001b[0;32m     19\u001b[0m                     np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mlongdouble,\n\u001b[0;32m     20\u001b[0m                     np\u001b[38;5;241m.\u001b[39mcomplex64, np\u001b[38;5;241m.\u001b[39mcomplex128, np\u001b[38;5;241m.\u001b[39mclongdouble]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\_lib\\_util.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeAlias, TypeVar\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, xp_size\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n\u001b[0;32m     17\u001b[0m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\_lib\\_array_api.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnpt\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     is_array_api_obj,\n\u001b[0;32m     20\u001b[0m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[0;32m     21\u001b[0m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[0;32m     22\u001b[0m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[0;32m     23\u001b[0m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[0;32m     24\u001b[0m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[0;32m     25\u001b[0m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[0;32m     26\u001b[0m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[0;32m     27\u001b[0m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_asarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray_namespace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_almost_equal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_array_almost_equal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_xp_devices\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxp_take_along_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxp_unsupported_param_msg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxp_vector_norm\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     39\u001b[0m ]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\_lib\\array_api_compat\\numpy\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# These imports may overwrite names from the import * above.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aliases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Don't know why, but we have to do an absolute import to import linalg. If we\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# instead do\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# It doesn't overwrite np.linalg from above. The import is generated\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# dynamically so that the library can be vendored.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28m__import__\u001b[39m(__package__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.linalg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\_lib\\array_api_compat\\numpy\\_aliases.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _aliases\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_xp\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_info\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __array_namespace_info__\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\_lib\\array_api_compat\\common\\_aliases.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NamedTuple\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, _check_device, device, is_torch_array, is_cupy_namespace\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# These functions are modified from the NumPy versions.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Creation functions add the device keyword (which does nothing for NumPy)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21marange\u001b[39m(\n\u001b[0;32m     22\u001b[0m     start: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m],\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     31\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ndarray:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_torch_array' from 'scipy._lib.array_api_compat.common._helpers' (c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\_lib\\array_api_compat\\common\\_helpers.py)"
     ]
    }
   ],
   "source": [
    "# Topic Modeling - Import libraries\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"Topic modeling libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff76260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Text Preprocessing for LDA\n",
    "def preprocess_for_lda(documents, min_word_len=2, max_word_len=15):\n",
    "    \"\"\"Preprocess documents for LDA topic modeling.\"\"\"\n",
    "    \n",
    "    print(\"Preprocessing documents for LDA...\")\n",
    "    \n",
    "    # Get English stopwords and add custom ones\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {\n",
    "        'like', 'really', 'good', 'great', 'nice', 'love', 'amazing', 'awesome',\n",
    "        'video', 'watch', 'see', 'look', 'get', 'go', 'come', 'make', 'take',\n",
    "        'title', 'description', 'comments', 'comment', 'say', 'said', 'says',\n",
    "        'youtube', 'channel', 'subscribe', 'notification', 'bell', 'hit',\n",
    "        'first', 'second', 'minute', 'time', 'day', 'year', 'way', 'thing',\n",
    "        'people', 'person', 'guy', 'girl', 'man', 'woman', 'one', 'two'\n",
    "    }\n",
    "    stop_words.update(custom_stopwords)\n",
    "    \n",
    "    # Tokenize and clean\n",
    "    processed_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "        \n",
    "        # Filter tokens\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            # Remove punctuation, numbers, short/long words, stopwords\n",
    "            if (token.isalpha() and \n",
    "                len(token) >= min_word_len and \n",
    "                len(token) <= max_word_len and \n",
    "                token not in stop_words):\n",
    "                filtered_tokens.append(token)\n",
    "        \n",
    "        processed_docs.append(filtered_tokens)\n",
    "    \n",
    "    # Remove documents that are too short\n",
    "    min_doc_length = 5\n",
    "    processed_docs = [doc for doc in processed_docs if len(doc) >= min_doc_length]\n",
    "    \n",
    "    print(f\"Preprocessed {len(processed_docs)} documents\")\n",
    "    print(f\"Average tokens per document: {np.mean([len(doc) for doc in processed_docs]):.1f}\")\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "# Preprocess the pooled documents\n",
    "processed_docs = preprocess_for_lda(pooled_documents)\n",
    "\n",
    "# Create dictionary and corpus for Gensim\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Filter extreme cases\n",
    "dictionary.filter_extremes(\n",
    "    no_below=2,      # Remove words that appear in less than 2 documents\n",
    "    no_above=0.8,    # Remove words that appear in more than 80% of documents\n",
    "    keep_n=10000     # Keep only top 10000 most frequent words\n",
    ")\n",
    "\n",
    "print(f\"Dictionary size after filtering: {len(dictionary)} unique tokens\")\n",
    "\n",
    "# Create corpus (bag of words representation)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print(f\"Corpus created: {len(corpus)} documents\")\n",
    "print(f\"Sample document representation: {corpus[0][:10]}...\")  # Show first 10 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a92107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal number of topics...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Test different numbers of topics\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding optimal number of topics...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Test K values from 4 to 30 with step 2\u001b[39;00m\n\u001b[0;32m     44\u001b[0m model_list, coherence_values \u001b[38;5;241m=\u001b[39m compute_coherence_values(\n\u001b[0;32m     45\u001b[0m     dictionary\u001b[38;5;241m=\u001b[39mdictionary,\n\u001b[0;32m     46\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mcorpus, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m#inc 2\u001b[39;00m\n\u001b[0;32m     51\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: LDA Model Training with K Optimization\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"Compute coherence values for different numbers of topics.\"\"\"\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(f\"Training LDA model with {num_topics} topics...\")\n",
    "        \n",
    "        # Train LDA model\n",
    "        model = models.LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            random_state=42,\n",
    "            update_every=1,\n",
    "            chunksize=100,\n",
    "            passes=10,\n",
    "            alpha='auto',\n",
    "            per_word_topics=True\n",
    "        )\n",
    "        \n",
    "        model_list.append(model)\n",
    "        \n",
    "        # Calculate coherence\n",
    "        coherencemodel = CoherenceModel(\n",
    "            model=model, \n",
    "            texts=texts, \n",
    "            dictionary=dictionary, \n",
    "            coherence='c_v'\n",
    "        )\n",
    "        \n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        print(f\"Coherence score for {num_topics} topics: {coherence_values[-1]:.4f}\")\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Test different numbers of topics\n",
    "print(\"Finding optimal number of topics...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Test K values from 4 to 30 with step 2\n",
    "model_list, coherence_values = compute_coherence_values(\n",
    "    dictionary=dictionary,\n",
    "    corpus=corpus, \n",
    "    texts=processed_docs,\n",
    "    start=4,\n",
    "    limit=31,\n",
    "    step=2  #inc 2\n",
    ")\n",
    "\n",
    "optimization_time = time.time() - start_time\n",
    "print(f\"\\nModel optimization completed in {optimization_time:.2f} seconds\")\n",
    "\n",
    "# Find optimal number of topics\n",
    "topic_range = list(range(4, 31, 2))\n",
    "optimal_idx = np.argmax(coherence_values)\n",
    "optimal_k = topic_range[optimal_idx]\n",
    "optimal_coherence = coherence_values[optimal_idx]\n",
    "\n",
    "print(f\"\\nOptimal number of topics: {optimal_k}\")\n",
    "print(f\"Best coherence score: {optimal_coherence:.4f}\")\n",
    "\n",
    "# Plot coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_range, coherence_values, 'bo-')\n",
    "plt.axvline(x=optimal_k, color='red', linestyle='--', label=f'Optimal K = {optimal_k}')\n",
    "plt.xlabel('Number of Topics (K)')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.title('Topic Coherence Scores vs Number of Topics')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Select the best model\n",
    "best_lda_model = model_list[optimal_idx]\n",
    "print(f\"\\nSelected LDA model with {optimal_k} topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Topic Assignment - Label each video with dominant topic\n",
    "def get_dominant_topic(ldamodel, corpus, video_metadata):\n",
    "    \"\"\"Get dominant topic for each video document.\"\"\"\n",
    "    \n",
    "    video_topics = []\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        # Get topic distribution for this document\n",
    "        topic_distribution = ldamodel.get_document_topics(doc)\n",
    "        \n",
    "        if topic_distribution:\n",
    "            # Find dominant topic (highest probability)\n",
    "            dominant_topic = max(topic_distribution, key=lambda x: x[1])\n",
    "            topic_id, topic_prob = dominant_topic\n",
    "            \n",
    "            # Get video metadata\n",
    "            video_info = video_metadata[i]\n",
    "            \n",
    "            video_topics.append({\n",
    "                'videoId': video_info['videoId'],\n",
    "                'video_title': video_info['title'],\n",
    "                'num_comments': video_info['num_comments'],\n",
    "                'viewCount': video_info['viewCount'],\n",
    "                'dominant_topic': topic_id,\n",
    "                'topic_probability': topic_prob,\n",
    "                'document_length': video_info['document_length']\n",
    "            })\n",
    "        else:\n",
    "            # Handle case where no topics found\n",
    "            video_info = video_metadata[i]\n",
    "            video_topics.append({\n",
    "                'videoId': video_info['videoId'],\n",
    "                'video_title': video_info['title'],\n",
    "                'num_comments': video_info['num_comments'],\n",
    "                'viewCount': video_info['viewCount'],\n",
    "                'dominant_topic': -1,  # No topic assigned\n",
    "                'topic_probability': 0.0,\n",
    "                'document_length': video_info['document_length']\n",
    "            })\n",
    "    \n",
    "    return video_topics\n",
    "\n",
    "# Assign topics to videos\n",
    "print(\"Assigning dominant topics to videos...\")\n",
    "video_topics = get_dominant_topic(best_lda_model, corpus, video_metadata)\n",
    "\n",
    "# Convert to DataFrame\n",
    "video_topics_df = pd.DataFrame(video_topics)\n",
    "\n",
    "print(f\"Assigned topics to {len(video_topics_df)} videos\")\n",
    "print(f\"Videos without topic assignment: {(video_topics_df['dominant_topic'] == -1).sum()}\")\n",
    "\n",
    "# Show topic distribution across videos\n",
    "topic_distribution = video_topics_df['dominant_topic'].value_counts().sort_index()\n",
    "print(f\"\\nTopic distribution across videos:\")\n",
    "for topic_id, count in topic_distribution.items():\n",
    "    if topic_id != -1:\n",
    "        percentage = count / len(video_topics_df) * 100\n",
    "        print(f\"  Topic {topic_id}: {count} videos ({percentage:.1f}%)\")\n",
    "\n",
    "# Show sample video assignments\n",
    "print(f\"\\nSample video topic assignments:\")\n",
    "print(\"=\"*100)\n",
    "sample_assignments = video_topics_df.head(10)[['videoId', 'video_title', 'dominant_topic', 'topic_probability', 'num_comments']]\n",
    "for _, row in sample_assignments.iterrows():\n",
    "    title = row['video_title'][:50] + \"...\" if len(str(row['video_title'])) > 50 else row['video_title']\n",
    "    print(f\"Video: {title}\")\n",
    "    print(f\"  Topic: {row['dominant_topic']} (confidence: {row['topic_probability']:.3f}, {row['num_comments']} comments)\")\n",
    "    print()\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb344dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract Topic Keywords with Frequencies\n",
    "def extract_topic_keywords(ldamodel, dictionary, num_words=20):\n",
    "    \"\"\"Extract keywords and their probabilities for each topic.\"\"\"\n",
    "    \n",
    "    topic_keywords = []\n",
    "    \n",
    "    for topic_id in range(ldamodel.num_topics):\n",
    "        # Get top words for this topic\n",
    "        topic_words = ldamodel.show_topic(topic_id, topn=num_words)\n",
    "        \n",
    "        for word, probability in topic_words:\n",
    "            # Get word frequency in corpus\n",
    "            word_id = dictionary.token2id.get(word, -1)\n",
    "            word_frequency = dictionary.dfs.get(word_id, 0) if word_id != -1 else 0\n",
    "            \n",
    "            topic_keywords.append({\n",
    "                'topic_id': topic_id,\n",
    "                'keyword': word,\n",
    "                'probability': probability,\n",
    "                'frequency': word_frequency,\n",
    "                'rank': len([x for x in topic_words[:topic_words.index((word, probability)) + 1]])\n",
    "            })\n",
    "    \n",
    "    return topic_keywords\n",
    "\n",
    "# Extract keywords for all topics\n",
    "print(\"Extracting topic keywords...\")\n",
    "topic_keywords = extract_topic_keywords(best_lda_model, dictionary, num_words=15)\n",
    "\n",
    "# Convert to DataFrame\n",
    "topic_keywords_df = pd.DataFrame(topic_keywords)\n",
    "\n",
    "print(f\"Extracted {len(topic_keywords_df)} keywords across {optimal_k} topics\")\n",
    "\n",
    "# Display topics with their top keywords\n",
    "print(f\"\\n=== DISCOVERED TOPICS ===\")\n",
    "for topic_id in range(optimal_k):\n",
    "    topic_data = topic_keywords_df[topic_keywords_df['topic_id'] == topic_id]\n",
    "    top_keywords = topic_data.head(10)\n",
    "    \n",
    "    # Count videos in this topic\n",
    "    videos_in_topic = (video_topics_df['dominant_topic'] == topic_id).sum()\n",
    "    \n",
    "    print(f\"\\nTopic {topic_id} ({videos_in_topic} videos):\")\n",
    "    keywords_list = []\n",
    "    for _, row in top_keywords.iterrows():\n",
    "        keywords_list.append(f\"{row['keyword']}({row['probability']:.3f})\")\n",
    "    \n",
    "    print(f\"  Keywords: {', '.join(keywords_list)}\")\n",
    "    \n",
    "    # Show example videos for this topic\n",
    "    example_videos = video_topics_df[video_topics_df['dominant_topic'] == topic_id].head(3)\n",
    "    if not example_videos.empty:\n",
    "        print(f\"  Example videos:\")\n",
    "        for _, video in example_videos.iterrows():\n",
    "            title = video['video_title'][:60] + \"...\" if len(str(video['video_title'])) > 60 else video['video_title']\n",
    "            print(f\"    - {title} (confidence: {video['topic_probability']:.3f})\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeca474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results and Create Visualizations\n",
    "import joblib\n",
    "\n",
    "print(\"Exporting topic modeling results...\")\n",
    "\n",
    "# 1. Export Video Topics CSV\n",
    "video_topics_df.to_csv('video_topics_assignment.csv', index=False, encoding='utf-8')\n",
    "print(f\"✅ Exported video topic assignments: video_topics_assignment.csv\")\n",
    "\n",
    "# 2. Export Topic Keywords CSV  \n",
    "topic_keywords_df.to_csv('topic_keywords_frequencies.csv', index=False, encoding='utf-8')\n",
    "print(f\"✅ Exported topic keywords: topic_keywords_frequencies.csv\")\n",
    "\n",
    "# 3. Create Summary Statistics\n",
    "summary_stats = {\n",
    "    'total_videos': len(video_topics_df),\n",
    "    'total_topics': optimal_k,\n",
    "    'avg_topic_probability': video_topics_df['topic_probability'].mean(),\n",
    "    'total_pooled_documents': len(pooled_documents),\n",
    "    'avg_comments_per_video': video_topics_df['num_comments'].mean(),\n",
    "    'coherence_score': optimal_coherence\n",
    "}\n",
    "\n",
    "print(f\"\\n=== TOPIC MODELING SUMMARY ===\")\n",
    "print(f\"Total Videos Analyzed: {summary_stats['total_videos']:,}\")\n",
    "print(f\"Number of Topics Discovered: {summary_stats['total_topics']}\")\n",
    "print(f\"Average Topic Confidence: {summary_stats['avg_topic_probability']:.3f}\")\n",
    "print(f\"Pooled Documents Created: {summary_stats['total_pooled_documents']:,}\")\n",
    "print(f\"Average Comments per Video: {summary_stats['avg_comments_per_video']:.1f}\")\n",
    "print(f\"Model Coherence Score: {summary_stats['coherence_score']:.4f}\")\n",
    "\n",
    "# 4. Create Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Topic Distribution\n",
    "topic_counts = video_topics_df['dominant_topic'].value_counts().sort_index()\n",
    "axes[0, 0].bar(range(len(topic_counts)), topic_counts.values, color='skyblue', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Topic ID')\n",
    "axes[0, 0].set_ylabel('Number of Videos')\n",
    "axes[0, 0].set_title('Videos per Topic')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Topic Confidence Distribution\n",
    "axes[0, 1].hist(video_topics_df['topic_probability'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Topic Probability')\n",
    "axes[0, 1].set_ylabel('Number of Videos')\n",
    "axes[0, 1].set_title('Topic Assignment Confidence')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Comments vs Topic Confidence\n",
    "scatter = axes[1, 0].scatter(video_topics_df['num_comments'], video_topics_df['topic_probability'], \n",
    "                            c=video_topics_df['dominant_topic'], cmap='tab10', alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Number of Comments')\n",
    "axes[1, 0].set_ylabel('Topic Probability')\n",
    "axes[1, 0].set_title('Comments vs Topic Confidence')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label='Topic ID')\n",
    "\n",
    "# Plot 4: Top Keywords Frequency\n",
    "top_keywords_overall = topic_keywords_df.groupby('keyword')['frequency'].sum().sort_values(ascending=False).head(15)\n",
    "axes[1, 1].barh(range(len(top_keywords_overall)), top_keywords_overall.values, color='coral', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(top_keywords_overall)))\n",
    "axes[1, 1].set_yticklabels(top_keywords_overall.index)\n",
    "axes[1, 1].set_xlabel('Total Frequency')\n",
    "axes[1, 1].set_title('Most Frequent Keywords Across All Topics')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Save the trained model to model folder\n",
    "import os\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "model_dir = 'model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save LDA model artifacts\n",
    "model_artifacts = {\n",
    "    'lda_model': best_lda_model,\n",
    "    'dictionary': dictionary,\n",
    "    'optimal_k': optimal_k,\n",
    "    'coherence_score': optimal_coherence,\n",
    "    'topic_keywords': topic_keywords_df,\n",
    "    'video_topics': video_topics_df,\n",
    "    'summary_stats': summary_stats,\n",
    "    'pooled_df': pooled_df\n",
    "}\n",
    "\n",
    "lda_model_path = os.path.join(model_dir, 'lda_topic_model.pkl')\n",
    "joblib.dump(model_artifacts, lda_model_path)\n",
    "print(f\"✅ Saved LDA model: {lda_model_path}\")\n",
    "\n",
    "# Also save individual model components for easier loading\n",
    "best_lda_model.save(os.path.join(model_dir, 'lda_model_gensim'))\n",
    "dictionary.save(os.path.join(model_dir, 'lda_dictionary.dict'))\n",
    "print(f\"✅ Saved Gensim LDA model: {os.path.join(model_dir, 'lda_model_gensim')}\")\n",
    "print(f\"✅ Saved LDA dictionary: {os.path.join(model_dir, 'lda_dictionary.dict')}\")\n",
    "\n",
    "print(f\"\\n🎉 Topic modeling completed! Files created:\")\n",
    "print(f\"   📁 video_topics_assignment.csv - Video topic labels\")\n",
    "print(f\"   📁 topic_keywords_frequencies.csv - Topic keywords with frequencies\") \n",
    "print(f\"   📁 lda_topic_model.pkl - Trained model for future use\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
